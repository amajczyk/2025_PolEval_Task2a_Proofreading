{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bcafb2",
   "metadata": {},
   "source": [
    "# Multi-Task Fine-tuning: Translation + Proofreading\n",
    "\n",
    "This notebook takes the already fine-tuned proofreading model and adds translation capability with a short, low learning rate fine-tuning.\n",
    "\n",
    "**Strategy:**\n",
    "- Start from the best proofreading checkpoint\n",
    "- Train on translation data with a small learning rate (to preserve proofreading knowledge)\n",
    "- Short training (1 epoch) to avoid catastrophic forgetting\n",
    "- The model should then be able to do both tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e9f70",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Base model - using the best proofreading checkpoint\n",
    "BASE_MODEL_SIZE = \"8B\"\n",
    "BASE_LORA_RANK = 64\n",
    "BASE_EPOCHS = 2\n",
    "BASE_BATCH_SIZE = 1\n",
    "BASE_GRADIENT_ACCUMULATION_STEPS = 2\n",
    "BASE_LEARNING_RATE = 2e-4\n",
    "BASE_WARMUP_STEPS = 10\n",
    "BASE_MAX_SEQ_LENGTH = 4096\n",
    "\n",
    "# Path to the proofreading model\n",
    "PROOFREADING_MODEL_PATH = f\"../../../outputs/qwen3_{BASE_MODEL_SIZE}_polish_inclusive_proofreading_lora_r{BASE_LORA_RANK}_lr{BASE_LEARNING_RATE}_ep{BASE_EPOCHS}_bs{BASE_BATCH_SIZE}_ga{BASE_GRADIENT_ACCUMULATION_STEPS}_warmup{BASE_WARMUP_STEPS}_seq{BASE_MAX_SEQ_LENGTH}/checkpoint-23000\"\n",
    "PROOFREADING_MODEL_PATH = os.path.abspath(PROOFREADING_MODEL_PATH)\n",
    "\n",
    "print(f\"Starting from proofreading model: {PROOFREADING_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75111a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for translation fine-tuning\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "\n",
    "# LoRA configuration - keep same rank as proofreading\n",
    "LORA_RANK = 64\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "# Training hyperparameters - SMALL learning rate to preserve proofreading knowledge\n",
    "LEARNING_RATE = 1e-4  # Much smaller than proofreading (2e-4)\n",
    "EPOCHS = 1  # Just 1 epoch to add translation capability\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 4\n",
    "WARMUP_STEPS = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"../../../outputs/qwen3_{BASE_MODEL_SIZE}_multitask_proofreading+translation_lora_r{LORA_RANK}_lr{LEARNING_RATE}_ep{EPOCHS}_bs{BATCH_SIZE}_ga{GRADIENT_ACCUMULATION_STEPS}_warmup{WARMUP_STEPS}_seq{MAX_SEQ_LENGTH}\"\n",
    "OUTPUT_DIR = os.path.abspath(OUTPUT_DIR)\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = \"../../../data/taskB/train.jsonl\"\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Training data: {TRAIN_DATA_PATH}\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE} (5x lower than proofreading)\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928e6b6",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e49a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cache directories\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/home/adam/Downloads/poleval-gender-new/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/adam/Downloads/poleval-gender-new/.cache/huggingface/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/adam/Downloads/poleval-gender-new/.cache/huggingface/datasets'\n",
    "os.environ['TRITON_CACHE_DIR'] = '/home/adam/Downloads/poleval-gender-new/.cache/triton'\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40a2c4",
   "metadata": {},
   "source": [
    "### Load the Proofreading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756add03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(\"Loading proofreading model...\")\n",
    "print(f\"From: {PROOFREADING_MODEL_PATH}\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = PROOFREADING_MODEL_PATH,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None,  # Auto-detect\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "print(\"✓ Proofreading model loaded!\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d126975",
   "metadata": {},
   "source": [
    "### Continue Training with Existing LoRA Adapters\n",
    "\n",
    "The model already has LoRA adapters from proofreading training. We'll continue training with the same adapters to add translation capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model already has LoRA adapters from proofreading training\n",
    "# We don't need to add new ones - just continue training with the existing adapters\n",
    "# This will allow the model to learn translation while preserving proofreading knowledge\n",
    "\n",
    "print(\"✓ Model already has LoRA adapters - ready for continued training!\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b39e3",
   "metadata": {},
   "source": [
    "### Load Translation Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967efd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_translation_data(file_path):\n",
    "    \"\"\"Load translation training data.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            data.append({\n",
    "                'prompt': item['prompt'],\n",
    "                'source': item['source'],\n",
    "                'target': item['target'],\n",
    "                'prompt_language': item['prompt_language'],\n",
    "                'source_language': item['source_language'],\n",
    "                'target_language': item['target_language']\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Load training data\n",
    "train_data = load_translation_data(TRAIN_DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"  Direction: {train_data[0]['source_language']} → {train_data[0]['target_language']}\")\n",
    "print(f\"  Prompt: {train_data[0]['prompt'][:80]}...\")\n",
    "print(f\"  Source: {train_data[0]['source'][:80]}...\")\n",
    "print(f\"  Target: {train_data[0]['target'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce96155",
   "metadata": {},
   "source": [
    "### Load System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a312b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translation system prompts\n",
    "with open('../../../system_prompts/translation/system_prompt_en_translation', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT_EN = f.read().strip()\n",
    "\n",
    "with open('../../../system_prompts/translation/system_prompt_pl_translation', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT_PL = f.read().strip()\n",
    "\n",
    "print(\"System prompts loaded.\")\n",
    "print(f\"English prompt: {len(SYSTEM_PROMPT_EN)} chars\")\n",
    "print(f\"Polish prompt: {len(SYSTEM_PROMPT_PL)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd136b",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_translation_prompt(example):\n",
    "    \"\"\"Format a translation example with the appropriate system prompt.\"\"\"\n",
    "    # Select system prompt based on prompt language\n",
    "    system_prompt = SYSTEM_PROMPT_EN if example['prompt_language'] == 'EN' else SYSTEM_PROMPT_PL\n",
    "    \n",
    "    # Construct user message\n",
    "    user_message = example['prompt'] + example['source']\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": example['target']}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Convert to HuggingFace dataset and format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "train_dataset = train_dataset.map(format_translation_prompt, remove_columns=train_dataset.column_names)\n",
    "\n",
    "print(f\"Formatted {len(train_dataset)} training examples\")\n",
    "print(f\"\\nExample formatted text (first 500 chars):\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25038297",
   "metadata": {},
   "source": [
    "### Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=42,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Total epochs: {EPOCHS}\")\n",
    "print(f\"  - Estimated steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8f102",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    args=UnslothTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=\"mlflow\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5db4d",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "This will fine-tune the proofreading model on translation data with a small learning rate to add translation capability while preserving proofreading skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING MULTI-TASK FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Base model: Proofreading checkpoint\")\n",
    "print(f\"New task: Translation (PL⇄EN)\")\n",
    "print(f\"Strategy: Low LR ({LEARNING_RATE}), {EPOCHS} epoch\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6f264",
   "metadata": {},
   "source": [
    "### Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "print(f\"\\nThis model should now be able to:\")\n",
    "print(f\"  1. Perform gender-inclusive proofreading (original task)\")\n",
    "print(f\"  2. Translate PL⇄EN with gender inclusivity (new task)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783cff4",
   "metadata": {},
   "source": [
    "### Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c42f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poleval-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
