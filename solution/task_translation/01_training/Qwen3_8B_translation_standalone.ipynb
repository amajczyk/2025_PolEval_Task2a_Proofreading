{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d83cdd",
   "metadata": {},
   "source": [
    "# Translation Fine-tuning from Base Model\n",
    "\n",
    "This notebook trains a translation model from scratch, starting with the base Qwen2.5 model (not the proofreading checkpoint).\n",
    "\n",
    "**Strategy:**\n",
    "- Start from base Qwen2.5-8B-Instruct\n",
    "- Train on translation data only\n",
    "- Focus solely on gender-sensitive PL⇄EN translation\n",
    "- This is a single-task model (translation only, no proofreading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a0e35",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24824780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Base model - same as proofreading training\n",
    "BASE_MODEL = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\"\n",
    "MODEL_SIZE = \"8B\"\n",
    "\n",
    "# Training configuration\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "\n",
    "# LoRA configuration - same as proofreading\n",
    "LORA_RANK = 64\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "# Training hyperparameters - same as proofreading\n",
    "LEARNING_RATE = 2e-4\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Effective batch size = 2\n",
    "WARMUP_STEPS = 10\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 3407\n",
    "\n",
    "# Generation parameters for inference\n",
    "REPETITION_PENALTY = 1.1\n",
    "NO_REPEAT_NGRAM_SIZE = 3\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"../../../outputs/qwen3_{MODEL_SIZE}_translation_standalone_lora_r{LORA_RANK}_lr{LEARNING_RATE}_ep{EPOCHS}_bs{BATCH_SIZE}_ga{GRADIENT_ACCUMULATION_STEPS}_warmup{WARMUP_STEPS}_seq{MAX_SEQ_LENGTH}\"\n",
    "OUTPUT_DIR = os.path.abspath(OUTPUT_DIR)\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = \"../../../data/taskB/train.jsonl\"\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Training data: {TRAIN_DATA_PATH}\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Repetition penalty: {REPETITION_PENALTY}\")\n",
    "print(f\"  - No repeat ngram size: {NO_REPEAT_NGRAM_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566375bc",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6938c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cache directories\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/home/adam/Downloads/poleval-gender-new/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/adam/Downloads/poleval-gender-new/.cache/huggingface/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/adam/Downloads/poleval-gender-new/.cache/huggingface/datasets'\n",
    "os.environ['TRITON_CACHE_DIR'] = '/home/adam/Downloads/poleval-gender-new/.cache/triton'\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e7cbc",
   "metadata": {},
   "source": [
    "### Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90affe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None,  # Auto-detect\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "print(\"Base model loaded!\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1c7c8",
   "metadata": {},
   "source": [
    "### Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc04e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters to the model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters added!\")\n",
    "print(f\"LoRA rank: {LORA_RANK}\")\n",
    "print(f\"LoRA alpha: {LORA_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4084c9a",
   "metadata": {},
   "source": [
    "### Load Translation Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eaafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_translation_data(file_path):\n",
    "    \"\"\"Load translation training data.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            data.append({\n",
    "                'prompt': item['prompt'],\n",
    "                'source': item['source'],\n",
    "                'target': item['target'],\n",
    "                'prompt_language': item['prompt_language'],\n",
    "                'source_language': item['source_language'],\n",
    "                'target_language': item['target_language']\n",
    "            })\n",
    "    return data\n",
    "\n",
    "# Load training data\n",
    "train_data = load_translation_data(TRAIN_DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"  Direction: {train_data[0]['source_language']} → {train_data[0]['target_language']}\")\n",
    "print(f\"  Prompt: {train_data[0]['prompt'][:80]}...\")\n",
    "print(f\"  Source: {train_data[0]['source'][:80]}...\")\n",
    "print(f\"  Target: {train_data[0]['target'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d165eb",
   "metadata": {},
   "source": [
    "### Load System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aff08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translation system prompts\n",
    "with open('../../../system_prompts/translation/system_prompt_en_translation', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT_EN = f.read().strip()\n",
    "\n",
    "with open('../../../system_prompts/translation/system_prompt_pl_translation', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT_PL = f.read().strip()\n",
    "\n",
    "print(\"System prompts loaded.\")\n",
    "print(f\"English prompt: {len(SYSTEM_PROMPT_EN)} chars\")\n",
    "print(f\"Polish prompt: {len(SYSTEM_PROMPT_PL)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d10e3da",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_translation_prompt(example):\n",
    "    \"\"\"Format a translation example with the appropriate system prompt.\"\"\"\n",
    "    # Select system prompt based on prompt language\n",
    "    system_prompt = SYSTEM_PROMPT_EN if example['prompt_language'] == 'EN' else SYSTEM_PROMPT_PL\n",
    "    \n",
    "    # Construct user message\n",
    "    user_message = example['prompt'] + example['source']\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": example['target']}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Convert to HuggingFace dataset and format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "train_dataset = train_dataset.map(format_translation_prompt, remove_columns=train_dataset.column_names)\n",
    "\n",
    "print(f\"Formatted {len(train_dataset)} training examples\")\n",
    "print(f\"\\nExample formatted text (first 500 chars):\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7f399",
   "metadata": {},
   "source": [
    "### Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=42,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Total epochs: {EPOCHS}\")\n",
    "print(f\"  - Estimated steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff16944",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8409ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    args=UnslothTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=42,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=\"mlflow\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1798e2",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "\n",
    "This will train a translation-only model from the base Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING TRANSLATION TRAINING (STANDALONE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Task: Translation (PL⇄EN)\")\n",
    "print(f\"Strategy: Full training, {EPOCHS} epochs\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1f2a0",
   "metadata": {},
   "source": [
    "### Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd793b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save generation parameters to a config file\n",
    "import json\n",
    "generation_config = {\n",
    "    \"repetition_penalty\": REPETITION_PENALTY,\n",
    "    \"no_repeat_ngram_size\": NO_REPEAT_NGRAM_SIZE,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 4096\n",
    "}\n",
    "\n",
    "with open(os.path.join(final_model_path, \"generation_config_recommended.json\"), 'w') as f:\n",
    "    json.dump(generation_config, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "print(f\"Generation config saved\")\n",
    "print(f\"\\nThis model is specialized for:\")\n",
    "print(f\"  - Gender-sensitive Polish ⇄ English translation\")\n",
    "print(f\"\\nRecommended inference parameters:\")\n",
    "print(f\"  - repetition_penalty: {REPETITION_PENALTY}\")\n",
    "print(f\"  - no_repeat_ngram_size: {NO_REPEAT_NGRAM_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec579b4",
   "metadata": {},
   "source": [
    "### Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f71797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907bb0e",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Let's quickly test the model on a few examples to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test on a couple of examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"prompt\": \"Translate the following text from Polish to English:\\n\",\n",
    "        \"source\": \"Wszyscy studenci i studentki muszą zdać egzamin.\",\n",
    "        \"prompt_language\": \"EN\",\n",
    "        \"source_language\": \"PL\",\n",
    "        \"target_language\": \"EN\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Przetłumacz poniższy tekst z angielskiego na polski:\\n\",\n",
    "        \"source\": \"All the teachers must attend the meeting.\",\n",
    "        \"prompt_language\": \"PL\",\n",
    "        \"source_language\": \"EN\",\n",
    "        \"target_language\": \"PL\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING MODEL ON SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, example in enumerate(test_examples):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i+1}: {example['source_language']} → {example['target_language']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Select system prompt\n",
    "    system_prompt = SYSTEM_PROMPT_EN if example['prompt_language'] == 'EN' else SYSTEM_PROMPT_PL\n",
    "    user_message = example['prompt'] + example['source']\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up artifacts\n",
    "    if \"<|im_end|>\" in response:\n",
    "        response = response.split(\"<|im_end|>\")[0]\n",
    "    if \"<|im_start|>\" in response:\n",
    "        response = response.split(\"<|im_start|>\")[-1]\n",
    "        if \"\\n\" in response:\n",
    "            response = response.split(\"\\n\", 1)[-1]\n",
    "    \n",
    "    print(f\"\\nSource: {example['source']}\")\n",
    "    print(f\"\\nTranslation: {response.strip()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poleval-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
