{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d113567f",
   "metadata": {},
   "source": [
    "# Gender-Inclusive Translation Inference - Test B\n",
    "\n",
    "This notebook performs inference for Task B (gender-sensitive Polish⇄English translation) using the model trained for proofreading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44979d33",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf84110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration - using the best proofreading model checkpoint\n",
    "MODEL_SIZE = \"8B\"\n",
    "LORA_RANK = 64\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 10\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "\n",
    "# Path to the trained proofreading model - using the best checkpoint\n",
    "MODEL_PATH = f\"../../../outputs/qwen3_{MODEL_SIZE}_polish_inclusive_proofreading_lora_r{LORA_RANK}_lr{LEARNING_RATE}_ep{EPOCHS}_bs{BATCH_SIZE}_ga{GRADIENT_ACCUMULATION_STEPS}_warmup{WARMUP_STEPS}_seq{MAX_SEQ_LENGTH}/checkpoint-23000\"\n",
    "MODEL_PATH = os.path.abspath(MODEL_PATH)\n",
    "print(f\"Using model from: {MODEL_PATH}\")\n",
    "\n",
    "# Inference parameters\n",
    "TEMPERATURE = 0.3  # Lower temperature for more precise translations\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "MAX_NEW_TOKENS = 4096  # Enough for longer texts\n",
    "\n",
    "# File paths\n",
    "TEST_FILE = \"../../../data/taskB/test_B.jsonl\"\n",
    "OUTPUT_FILE = \"predictions_translation_test_B.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc24955",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "\n",
    "**IMPORTANT:** After running this cell, you MUST restart the kernel for the cache paths to take effect. Then run all cells from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix HuggingFace cache permissions\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/media/adam/NVME_500/poleval-gender-new/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/media/adam/NVME_500/poleval-gender-new/.cache/huggingface/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/media/adam/NVME_500/poleval-gender-new/.cache/huggingface/datasets'\n",
    "os.environ['TRITON_CACHE_DIR'] = '/media/adam/NVME_500/poleval-gender-new/.cache/triton'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060560e",
   "metadata": {},
   "source": [
    "### Load the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Enable inference mode for 2x faster generation\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f3fed",
   "metadata": {},
   "source": [
    "### Load System Prompts\n",
    "\n",
    "We'll load both English and Polish system prompts and select the appropriate one based on the prompt_language field in each test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both English and Polish translation system prompts\n",
    "with open('../../../system_prompts/translation/system_prompt_en_translation', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT_EN = f.read().strip()\n",
    "\n",
    "with open('../../../system_prompts/translation/system_prompt_pl_translation', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT_PL = f.read().strip()\n",
    "\n",
    "print(\"System prompts loaded.\")\n",
    "print(f\"English system prompt length: {len(SYSTEM_PROMPT_EN)} characters\")\n",
    "print(f\"Polish system prompt length: {len(SYSTEM_PROMPT_PL)} characters\")\n",
    "print(f\"\\nFirst 200 characters of English prompt:\")\n",
    "print(SYSTEM_PROMPT_EN[:200] + \"...\")\n",
    "print(f\"\\nFirst 200 characters of Polish prompt:\")\n",
    "print(SYSTEM_PROMPT_PL[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76ce43",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac32abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load test data\n",
    "test_data = load_jsonl(TEST_FILE)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test examples from {TEST_FILE}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"IPIS ID: {test_data[0]['ipis_id']}\")\n",
    "print(f\"Prompt: {test_data[0]['prompt']}\")\n",
    "print(f\"Source language: {test_data[0]['source_language']}\")\n",
    "print(f\"Target language: {test_data[0]['target_language']}\")\n",
    "print(f\"Source (first 200 chars): {test_data[0]['source'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9328c9c",
   "metadata": {},
   "source": [
    "### Analyze Dataset\n",
    "\n",
    "Let's check the distribution of translation directions in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count translation directions\n",
    "translation_directions = Counter(\n",
    "    f\"{item['source_language']} → {item['target_language']}\" \n",
    "    for item in test_data\n",
    ")\n",
    "\n",
    "print(\"Translation directions in test set:\")\n",
    "for direction, count in translation_directions.items():\n",
    "    print(f\"  {direction}: {count} examples ({count/len(test_data)*100:.1f}%)\")\n",
    "\n",
    "# Count prompt languages\n",
    "prompt_languages = Counter(item['prompt_language'] for item in test_data)\n",
    "print(f\"\\nPrompt languages:\")\n",
    "for lang, count in prompt_languages.items():\n",
    "    print(f\"  {lang}: {count} examples ({count/len(test_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb6fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14515a0b",
   "metadata": {},
   "source": [
    "### Try Base Model (Without Fine-tuning)\n",
    "\n",
    "The proofreading model wasn't trained for translation. Let's try the base Qwen model to see if it has translation capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca1713",
   "metadata": {},
   "source": [
    "### Generate Predictions\n",
    "\n",
    "This will generate translations for all texts in the test set, handling both PL→EN and EN→PL directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dff0dd",
   "metadata": {},
   "source": [
    "### Test Translation (Single Example)\n",
    "\n",
    "Let's test the translation on a single example first to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a33e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a few examples from different translation directions\n",
    "test_indices = [0]  # First 3 examples\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING TRANSLATION ON SAMPLE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for test_idx in test_indices:\n",
    "    item = test_data[test_idx]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {test_idx + 1}: {item['ipis_id']}\")\n",
    "    print(f\"Direction: {item['source_language']} → {item['target_language']}\")\n",
    "    print(f\"Prompt language: {item['prompt_language']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Construct the prompt\n",
    "    user_message = item['prompt'] + item['source']\n",
    "    \n",
    "    # Select appropriate system prompt\n",
    "    system_prompt = SYSTEM_PROMPT_EN if item['prompt_language'] == 'EN' else SYSTEM_PROMPT_PL\n",
    "    \n",
    "    print(f\"\\nSystem prompt (first 150 chars):\")\n",
    "    print(system_prompt[:150] + \"...\")\n",
    "    \n",
    "    print(f\"\\nUser message (first 200 chars):\")\n",
    "    print(user_message[:200] + \"...\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFull prompt length: {len(text)} characters\")\n",
    "    print(f\"\\nFull prompt (last 300 chars before generation):\")\n",
    "    print(\"...\" + text[-300:])\n",
    "    \n",
    "    # Tokenize and move to GPU\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            do_sample=True if TEMPERATURE > 0 else False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up artifacts\n",
    "    if \"<|im_end|>\" in response:\n",
    "        response = response.split(\"<|im_end|>\")[0]\n",
    "    if \"<|im_start|>\" in response:\n",
    "        response = response.split(\"<|im_start|>\")[-1]\n",
    "        if \"\\n\" in response:\n",
    "            response = response.split(\"\\n\", 1)[-1]\n",
    "    \n",
    "    print(f\"\\nGENERATED TRANSLATION:\")\n",
    "    print(response.strip())\n",
    "    \n",
    "    print(f\"\\nStats:\")\n",
    "    print(f\"  - Source length: {len(item['source'])} chars\")\n",
    "    print(f\"  - Target length: {len(response.strip())} chars\")\n",
    "    print(f\"  - Is same as source: {response.strip() == item['source']}\")\n",
    "    \n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = \"inference_checkpoints_translation\"\n",
    "SAVE_INTERVAL = 10\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Check for existing checkpoints to resume from\n",
    "checkpoint_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"predictions_checkpoint_\")])\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "    print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Loading predictions from checkpoint...\")\n",
    "    \n",
    "    with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "    \n",
    "    processed_ids = {p['ipis_id'] for p in predictions}\n",
    "    start_idx = len(predictions)\n",
    "    print(f\"Resuming from example {start_idx} ({len(predictions)} already processed)\")\n",
    "else:\n",
    "    predictions = []\n",
    "    processed_ids = set()\n",
    "    start_idx = 0\n",
    "    print(\"Starting from scratch\")\n",
    "\n",
    "print(f\"\\nGenerating predictions for {len(test_data)} examples...\")\n",
    "print(f\"Parameters: temperature={TEMPERATURE}, top_p={TOP_P}, max_new_tokens={MAX_NEW_TOKENS}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    for idx, item in enumerate(tqdm(test_data[start_idx:], initial=start_idx, total=len(test_data), desc=\"Generating translations\")):\n",
    "        # Skip if already processed\n",
    "        if item['ipis_id'] in processed_ids:\n",
    "            continue\n",
    "        \n",
    "        # Construct the prompt using the same format as training\n",
    "        user_message = item['prompt'] + item['source']\n",
    "        \n",
    "        # Select appropriate system prompt based on prompt_language\n",
    "        system_prompt = SYSTEM_PROMPT_EN if item['prompt_language'] == 'EN' else SYSTEM_PROMPT_PL\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and move to GPU\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                top_k=TOP_K,\n",
    "                do_sample=True if TEMPERATURE > 0 else False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated tokens (excluding the input prompt)\n",
    "        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up any remaining chat template artifacts\n",
    "        if \"<|im_end|>\" in response:\n",
    "            response = response.split(\"<|im_end|>\")[0]\n",
    "        if \"<|im_start|>\" in response:\n",
    "            response = response.split(\"<|im_start|>\")[-1]\n",
    "            if \"\\n\" in response:\n",
    "                response = response.split(\"\\n\", 1)[-1]\n",
    "        \n",
    "        # Create prediction entry matching the expected format for Task B\n",
    "        prediction = {\n",
    "            \"source_resource_id\": item['source_resource_id'],\n",
    "            \"ipis_id\": item['ipis_id'],\n",
    "            \"prompt\": item['prompt'],\n",
    "            \"source\": item['source'],\n",
    "            \"target\": response.strip(),\n",
    "            \"prompt_language\": item['prompt_language'],\n",
    "            \"source_language\": item['source_language'],\n",
    "            \"target_language\": item['target_language']\n",
    "        }\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        processed_ids.add(item['ipis_id'])\n",
    "        \n",
    "        # Save checkpoint every SAVE_INTERVAL examples\n",
    "        if len(predictions) % SAVE_INTERVAL == 0:\n",
    "            checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"predictions_checkpoint_{len(predictions):05d}.jsonl\")\n",
    "            with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                for pred in predictions:\n",
    "                    f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "            print(f\"\\n✓ Checkpoint saved: {checkpoint_file} ({len(predictions)} predictions)\")\n",
    "            print(f\"Direction: {prediction['source_language']} → {prediction['target_language']}\")\n",
    "            print(f\"Latest translation (first 150 chars): {response.strip()[:150]}...\")\n",
    "\n",
    "    print(f\"\\n✓ Generated {len(predictions)} translations successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError occurred: {e}\")\n",
    "    print(f\"Predictions saved up to example {len(predictions)}\")\n",
    "    print(f\"To resume, simply re-run this cell - it will load from the last checkpoint\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58aa4e8",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402699f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to JSONL file with .tsv extension for PolEval submission\n",
    "output_file_jsonl = OUTPUT_FILE\n",
    "output_file_tsv = OUTPUT_FILE.replace('.jsonl', '.tsv')\n",
    "\n",
    "# Save as .jsonl for reference\n",
    "with open(output_file_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Save as .tsv for PolEval submission\n",
    "with open(output_file_tsv, 'w', encoding='utf-8') as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"✓ Predictions saved to:\")\n",
    "print(f\"  - {output_file_jsonl}\")\n",
    "print(f\"  - {output_file_tsv} (for PolEval submission)\")\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df79a0b",
   "metadata": {},
   "source": [
    "### Preview Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions for both translation directions\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find examples of each translation direction\n",
    "pl_to_en = [p for p in predictions if p['source_language'] == 'PL' and p['target_language'] == 'EN']\n",
    "en_to_pl = [p for p in predictions if p['source_language'] == 'EN' and p['target_language'] == 'PL']\n",
    "\n",
    "if pl_to_en:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POLISH → ENGLISH EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    for i in range(min(2, len(pl_to_en))):\n",
    "        pred = pl_to_en[i]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"IPIS ID: {pred['ipis_id']}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"SOURCE (Polish, first 250 chars):\\n{pred['source'][:250]}...\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"TARGET (English, first 250 chars):\\n{pred['target'][:250]}...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "if en_to_pl:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENGLISH → POLISH EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    for i in range(min(2, len(en_to_pl))):\n",
    "        pred = en_to_pl[i]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"IPIS ID: {pred['ipis_id']}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"SOURCE (English, first 250 chars):\\n{pred['source'][:250]}...\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"TARGET (Polish, first 250 chars):\\n{pred['target'][:250]}...\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fbd9e",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics by translation direction\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print()\n",
    "\n",
    "# Statistics by translation direction\n",
    "directions = Counter(f\"{p['source_language']} → {p['target_language']}\" for p in predictions)\n",
    "print(\"Translation directions:\")\n",
    "for direction, count in directions.items():\n",
    "    print(f\"  {direction}: {count} examples ({count/len(predictions)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Length statistics for PL → EN\n",
    "if pl_to_en:\n",
    "    source_lengths_pl_en = [len(p['source']) for p in pl_to_en]\n",
    "    target_lengths_pl_en = [len(p['target']) for p in pl_to_en]\n",
    "    \n",
    "    print(\"Polish → English:\")\n",
    "    print(f\"  Source (PL) length - Mean: {np.mean(source_lengths_pl_en):.1f}, Median: {np.median(source_lengths_pl_en):.1f}\")\n",
    "    print(f\"  Target (EN) length - Mean: {np.mean(target_lengths_pl_en):.1f}, Median: {np.median(target_lengths_pl_en):.1f}\")\n",
    "    print(f\"  Average length change: {np.mean(target_lengths_pl_en) - np.mean(source_lengths_pl_en):.1f} chars ({(np.mean(target_lengths_pl_en) / np.mean(source_lengths_pl_en) - 1) * 100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# Length statistics for EN → PL\n",
    "if en_to_pl:\n",
    "    source_lengths_en_pl = [len(p['source']) for p in en_to_pl]\n",
    "    target_lengths_en_pl = [len(p['target']) for p in en_to_pl]\n",
    "    \n",
    "    print(\"English → Polish:\")\n",
    "    print(f\"  Source (EN) length - Mean: {np.mean(source_lengths_en_pl):.1f}, Median: {np.median(source_lengths_en_pl):.1f}\")\n",
    "    print(f\"  Target (PL) length - Mean: {np.mean(target_lengths_en_pl):.1f}, Median: {np.median(target_lengths_en_pl):.1f}\")\n",
    "    print(f\"  Average length change: {np.mean(target_lengths_en_pl) - np.mean(source_lengths_en_pl):.1f} chars ({(np.mean(target_lengths_en_pl) / np.mean(source_lengths_en_pl) - 1) * 100:.1f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poleval-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
