{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feebae45",
   "metadata": {},
   "source": [
    "# Remove Unwanted Tokens from Translation Predictions\n",
    "\n",
    "This notebook cleans translation predictions by removing unwanted tokens that the model may generate, such as:\n",
    "- `<think>` tags and their contents\n",
    "- Other meta-tokens or reasoning artifacts\n",
    "\n",
    "These tokens should not be part of the actual translation output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72642d06",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Input and output files\n",
    "INPUT_FILE = \"../02_inference/predictions_translation_test_B_multitask.jsonl\"\n",
    "OUTPUT_FILE = \"../02_inference/predictions_translation_test_B_multitask_cleaned.jsonl\"\n",
    "\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f45f2a",
   "metadata": {},
   "source": [
    "### Define Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_translation(text):\n",
    "    \"\"\"\n",
    "    Remove unwanted tokens from translation output.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw translation output\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned translation\n",
    "    \"\"\"\n",
    "    # Remove <think> tags and their contents\n",
    "    # Pattern: <think> ... </think> (including multiline)\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove any remaining opening or closing think tags\n",
    "    text = text.replace('<think>', '').replace('</think>', '')\n",
    "    \n",
    "    # Remove other potential unwanted tokens at the start\n",
    "    # (add more patterns as needed)\n",
    "    unwanted_prefixes = [\n",
    "        'think',\n",
    "        'Think',\n",
    "        'Okay,',\n",
    "        'Sure,',\n",
    "        'Let me',\n",
    "        'I will',\n",
    "        'Here is',\n",
    "        'Translation:',\n",
    "        'Output:',\n",
    "    ]\n",
    "    \n",
    "    for prefix in unwanted_prefixes:\n",
    "        if text.strip().startswith(prefix):\n",
    "            # Remove the prefix and everything up to the first newline or colon\n",
    "            text = re.sub(rf'^{re.escape(prefix)}[^\\n:]*[:|\\n]?\\s*', '', text.strip())\n",
    "    \n",
    "    # Clean up excessive whitespace\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Normalize multiple newlines to maximum 2\n",
    "    text = re.sub(r'\\n\\n\\n+', '\\n\\n', text)\n",
    "    \n",
    "    # Remove any leading newlines\n",
    "    text = text.lstrip('\\n')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_cases = [\n",
    "    \"<think>\\n\\n</think>\\n\\nThis is the translation.\",\n",
    "    \"think about it\\n\\nThis is the translation.\",\n",
    "    \"Okay, here is the translation:\\nThis is the translation.\",\n",
    "    \"This is already clean.\",\n",
    "    \"<think>Some reasoning</think>\\n\\n\\nActual translation here.\",\n",
    "]\n",
    "\n",
    "print(\"Testing cleaning function:\")\n",
    "print(\"=\"*80)\n",
    "for i, test in enumerate(test_cases):\n",
    "    cleaned = clean_translation(test)\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(f\"  Input:  [{test[:60]}...]\")\n",
    "    print(f\"  Output: [{cleaned[:60]}...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e85a79",
   "metadata": {},
   "source": [
    "### Load and Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    predictions = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(predictions)} predictions\")\n",
    "\n",
    "# Analyze how many need cleaning\n",
    "needs_cleaning = 0\n",
    "for pred in predictions:\n",
    "    target = pred['target']\n",
    "    cleaned = clean_translation(target)\n",
    "    if target != cleaned:\n",
    "        needs_cleaning += 1\n",
    "\n",
    "print(f\"Predictions that need cleaning: {needs_cleaning} ({needs_cleaning/len(predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fa75d",
   "metadata": {},
   "source": [
    "### Show Examples Before/After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of cleaning\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLES OF CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "examples_shown = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    target = pred['target']\n",
    "    cleaned = clean_translation(target)\n",
    "    \n",
    "    if target != cleaned and examples_shown < 5:\n",
    "        examples_shown += 1\n",
    "        print(f\"\\nExample {examples_shown} (ID: {pred['ipis_id']}):\")\n",
    "        print(f\"Direction: {pred['source_language']} â†’ {pred['target_language']}\")\n",
    "        print(f\"\\nBEFORE (first 200 chars):\")\n",
    "        print(target[:200])\n",
    "        print(f\"\\nAFTER (first 200 chars):\")\n",
    "        print(cleaned[:200])\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    if examples_shown >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cbebf",
   "metadata": {},
   "source": [
    "### Clean All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all predictions\n",
    "cleaned_predictions = []\n",
    "cleaning_stats = {\n",
    "    'total': len(predictions),\n",
    "    'cleaned': 0,\n",
    "    'unchanged': 0,\n",
    "    'avg_length_before': 0,\n",
    "    'avg_length_after': 0,\n",
    "}\n",
    "\n",
    "total_length_before = 0\n",
    "total_length_after = 0\n",
    "\n",
    "for pred in predictions:\n",
    "    original_target = pred['target']\n",
    "    cleaned_target = clean_translation(original_target)\n",
    "    \n",
    "    total_length_before += len(original_target)\n",
    "    total_length_after += len(cleaned_target)\n",
    "    \n",
    "    if original_target != cleaned_target:\n",
    "        cleaning_stats['cleaned'] += 1\n",
    "    else:\n",
    "        cleaning_stats['unchanged'] += 1\n",
    "    \n",
    "    # Create new prediction with only required fields (matching sample_translation.tsv format)\n",
    "    cleaned_pred = {\n",
    "        'ipis_id': pred['ipis_id'],\n",
    "        'source': pred['source'],\n",
    "        'target': cleaned_target\n",
    "    }\n",
    "    \n",
    "    cleaned_predictions.append(cleaned_pred)\n",
    "\n",
    "cleaning_stats['avg_length_before'] = total_length_before / len(predictions)\n",
    "cleaning_stats['avg_length_after'] = total_length_after / len(predictions)\n",
    "\n",
    "print(\"Cleaning Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total predictions: {cleaning_stats['total']}\")\n",
    "print(f\"Cleaned: {cleaning_stats['cleaned']} ({cleaning_stats['cleaned']/cleaning_stats['total']*100:.1f}%)\")\n",
    "print(f\"Unchanged: {cleaning_stats['unchanged']} ({cleaning_stats['unchanged']/cleaning_stats['total']*100:.1f}%)\")\n",
    "print(f\"Avg length before: {cleaning_stats['avg_length_before']:.1f} chars\")\n",
    "print(f\"Avg length after: {cleaning_stats['avg_length_after']:.1f} chars\")\n",
    "print(f\"Avg reduction: {cleaning_stats['avg_length_before'] - cleaning_stats['avg_length_after']:.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4486442",
   "metadata": {},
   "source": [
    "### Save Cleaned Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9af8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned predictions as JSONL\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for pred in cleaned_predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Cleaned predictions saved to: {OUTPUT_FILE}\")\n",
    "print(f\"Total predictions: {len(cleaned_predictions)}\")\n",
    "\n",
    "# Also save as .tsv (same format, just different extension for PolEval submission)\n",
    "OUTPUT_FILE_TSV = OUTPUT_FILE.replace('.jsonl', '.tsv')\n",
    "with open(OUTPUT_FILE_TSV, 'w', encoding='utf-8') as f:\n",
    "    for pred in cleaned_predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Also saved as: {OUTPUT_FILE_TSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3a218",
   "metadata": {},
   "source": [
    "### Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify the cleaned file\n",
    "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    loaded_cleaned = [json.loads(line) for line in f]\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original predictions: {len(predictions)}\")\n",
    "print(f\"Cleaned predictions: {len(loaded_cleaned)}\")\n",
    "print(f\"Match: {len(predictions) == len(loaded_cleaned)}\")\n",
    "\n",
    "# Verify format matches sample_translation.tsv\n",
    "print(\"\\nFormat verification:\")\n",
    "expected_fields = ['ipis_id', 'source', 'target']\n",
    "if loaded_cleaned:\n",
    "    actual_fields = list(loaded_cleaned[0].keys())\n",
    "    print(f\"Expected fields: {expected_fields}\")\n",
    "    print(f\"Actual fields: {actual_fields}\")\n",
    "    print(f\"Format matches: {actual_fields == expected_fields}\")\n",
    "\n",
    "# Show a few random examples\n",
    "import random\n",
    "print(\"\\nRandom sample of cleaned predictions:\")\n",
    "print(\"=\"*80)\n",
    "for i in random.sample(range(len(loaded_cleaned)), min(3, len(loaded_cleaned))):\n",
    "    pred = loaded_cleaned[i]\n",
    "    print(f\"\\nID: {pred['ipis_id']}\")\n",
    "    print(f\"Source (first 100 chars): {pred['source'][:100]}...\")\n",
    "    print(f\"Target (first 150 chars): {pred['target'][:150]}...\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poleval-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
