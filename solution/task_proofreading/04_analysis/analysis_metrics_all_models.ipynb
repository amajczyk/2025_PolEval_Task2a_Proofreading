{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd114e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566f32e",
   "metadata": {},
   "source": [
    "# Comprehensive Model Performance Analysis\n",
    "\n",
    "This notebook calculates and visualizes performance metrics for all model variants across multiple inference passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea0238",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92108fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set publication-quality style for ACL paper\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 11\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "\n",
    "# Seaborn style for ACL papers\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ad958",
   "metadata": {},
   "source": [
    "## Define Metric Calculation Functions\n",
    "\n",
    "Based on the author's functions from `funkcje_pythonowe.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metric helper functions from the author's utilities\n",
    "import sys\n",
    "from pathlib import Path as _Path_for_import\n",
    "from importlib import reload\n",
    "\n",
    "# Ensure the project root is on sys.path so 'author_funcs' package can be imported\n",
    "_PROJECT_ROOT = _Path_for_import(\"/mnt/g/poleval-gender\")\n",
    "if str(_PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(_PROJECT_ROOT))\n",
    "\n",
    "# Import all needed functions from author_funcs.funkcje_pythonowe\n",
    "import author_funcs.funkcje_pythonowe as author_module\n",
    "reload(author_module)  # Reload in case we modified the file\n",
    "\n",
    "from author_funcs.funkcje_pythonowe import (\n",
    "    compare_with_gold,\n",
    "    read_json_instances,\n",
    "    filter_elements,\n",
    "    find_longest_list,\n",
    "    find_dc,\n",
    "    printing_results\n",
    ")\n",
    "\n",
    "# Wrapper for the author's printing_results to match our usage pattern\n",
    "def calculate_metrics(confusion_dict: Dict[str, int]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate evaluation metrics from confusion matrix using author's printing_results.\"\"\"\n",
    "    temp_dict = {}\n",
    "    result = printing_results(confusion_dict, 'temp', temp_dict)\n",
    "    return result\n",
    "\n",
    "print(\"Imported functions from author_funcs.funkcje_pythonowe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76523fce",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa984a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path(\"/mnt/g/poleval-gender\")\n",
    "INFERENCE_DIR = BASE_DIR / \"solution/task_proofreading/02_inference\"\n",
    "GOLD_STANDARD = BASE_DIR / \"data/taskA/test_gold_standard_normalised_B.jsonl\"\n",
    "\n",
    "# Output directory for plots\n",
    "OUTPUT_DIR = BASE_DIR / \"analysis_results\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Gold standard: {GOLD_STANDARD}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fe521",
   "metadata": {},
   "source": [
    "## Find All Normalized Prediction Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e267c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all normalized final prediction files (in pass_* and submission folders)\n",
    "normalized_files = sorted(\n",
    "    list(INFERENCE_DIR.glob(\"inference_checkpoints_*/pass_*/predictions_final_*_NORMALIZED.jsonl\")) +\n",
    "    list(INFERENCE_DIR.glob(\"inference_checkpoints_*/submission/predictions_final_*_NORMALIZED.jsonl\"))\n",
    ")\n",
    "\n",
    "print(f\"Found {len(normalized_files)} normalized prediction files:\")\n",
    "for f in normalized_files[:5]:\n",
    "    print(f\"  {f.relative_to(INFERENCE_DIR)}\")\n",
    "if len(normalized_files) > 5:\n",
    "    print(f\"  ... and {len(normalized_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e4d89",
   "metadata": {},
   "source": [
    "## Calculate Metrics for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(\"Calculating metrics for all models...\\n\")\n",
    "\n",
    "for pred_file in tqdm(normalized_files, desc=\"Processing models\"):\n",
    "    # Extract metadata from path\n",
    "    parts = pred_file.parts\n",
    "    \n",
    "    # Get checkpoint directory name (e.g., 'inference_checkpoints_lora_r64')\n",
    "    checkpoint_dir = [p for p in parts if p.startswith('inference_checkpoints_')][0]\n",
    "    \n",
    "    # Extract model configuration\n",
    "    if 'fulltext' in checkpoint_dir:\n",
    "        model_type = 'fulltext'\n",
    "        lora_rank = int(checkpoint_dir.split('_r')[-1])\n",
    "    else:\n",
    "        model_type = 'generated only'\n",
    "        lora_rank = int(checkpoint_dir.split('_r')[-1])\n",
    "    \n",
    "    # Get pass number (handle both pass_* and submission folders)\n",
    "    pass_dir_candidates = [p for p in parts if p.startswith('pass_') or p == 'submission']\n",
    "    if pass_dir_candidates:\n",
    "        pass_dir = pass_dir_candidates[0]\n",
    "        if pass_dir == 'submission':\n",
    "            pass_num = 'submission'\n",
    "        else:\n",
    "            pass_num = int(pass_dir.split('_')[-1])\n",
    "    else:\n",
    "        pass_num = 'unknown'\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    try:\n",
    "        confusion = compare_with_gold(str(GOLD_STANDARD), str(pred_file))\n",
    "        metrics = calculate_metrics(confusion)\n",
    "        \n",
    "        # Combine all information\n",
    "        result = {\n",
    "            'model_type': model_type,\n",
    "            'lora_rank': lora_rank,\n",
    "            'pass': pass_num,\n",
    "            'model_name': f\"LoRA-r{lora_rank}\",\n",
    "            'file_path': str(pred_file.relative_to(INFERENCE_DIR)),\n",
    "            **confusion,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pred_file.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessfully calculated metrics for {len(results)} model variants\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59071205",
   "metadata": {},
   "source": [
    "## Create Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by model type, lora rank, and pass (handle mixed int/str in pass column)\n",
    "df = df.sort_values(['model_type', 'lora_rank', 'pass'], key=lambda col: col if col.name != 'pass' else col.astype(str)).reset_index(drop=True)\n",
    "\n",
    "# Display summary\n",
    "print(\"Results DataFrame:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nModel types: {df['model_type'].unique()}\")\n",
    "print(f\"LoRA ranks: {sorted(df['lora_rank'].unique())}\")\n",
    "print(f\"Passes: {sorted(df['pass'].unique(), key=str)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "display(df[['model_type', 'lora_rank', 'pass', 'accuracy', 'precision', 'recall', 'f1']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only the missing \"generated only\" rank 64 pass 3 file (not fulltext)\n",
    "# Fulltext rank 64 has all 3 passes, but \"generated only\" is missing pass 3\n",
    "df = df[~((df['lora_rank'] == 64) & (df['pass'] == 3) & (df['model_type'] == 'generated only'))].reset_index(drop=True)\n",
    "print(f\"Filtered out missing 'generated only' rank 64 pass 3\")\n",
    "print(f\"Kept fulltext rank 64 pass 3\")\n",
    "print(f\"Total records: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0df504",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = OUTPUT_DIR / \"all_model_metrics.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2c974",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\nOverall Performance Metrics:\")\n",
    "print(df[['accuracy', 'precision', 'recall', 'f1', 'fnr']].describe())\n",
    "\n",
    "# Best models by F1 score\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 10 MODELS BY F1 SCORE\")\n",
    "print(\"=\" * 80)\n",
    "top_models = df.nlargest(10, 'f1')[['model_type', 'lora_rank', 'pass', 'f1', 'precision', 'recall', 'accuracy']]\n",
    "display(top_models)\n",
    "\n",
    "# Performance by LoRA rank\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE METRICS BY LORA RANK (with std dev for LaTeX tables)\")\n",
    "print(\"=\" * 80)\n",
    "by_rank = df.groupby(['lora_rank', 'model_type']).agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'model_type': 'count'  # Count instances\n",
    "})\n",
    "# Flatten column names\n",
    "by_rank.columns = ['_'.join(col).strip() if col[1] else col[0] for col in by_rank.columns.values]\n",
    "by_rank = by_rank.rename(columns={'model_type_count': 'n_instances'}).round(4)\n",
    "display(by_rank)\n",
    "\n",
    "# Performance by pass\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE METRICS BY PASS NUMBER (with std dev for LaTeX tables)\")\n",
    "print(\"=\" * 80)\n",
    "by_pass = df.groupby('pass').agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'model_type': 'count'  # Count instances\n",
    "})\n",
    "# Flatten column names\n",
    "by_pass.columns = ['_'.join(col).strip() if col[1] else col[0] for col in by_pass.columns.values]\n",
    "by_pass = by_pass.rename(columns={'model_type_count': 'n_instances'}).round(4)\n",
    "# Keep n_instances as integer\n",
    "by_pass['n_instances'] = by_pass['n_instances'].astype(int)\n",
    "display(by_pass)\n",
    "\n",
    "# Performance by model type\n",
    "if len(df['model_type'].unique()) > 1:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AVERAGE METRICS BY MODEL TYPE - Only rank 64 (with std dev for LaTeX tables)\")\n",
    "    print(\"=\" * 80)\n",
    "    df_r64 = df[df['lora_rank'] == 64]\n",
    "    by_type = df_r64.groupby('model_type').agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'lora_rank': 'count'  # Count instances\n",
    "    })\n",
    "    # Flatten column names\n",
    "    by_type.columns = ['_'.join(col).strip() if col[1] else col[0] for col in by_type.columns.values]\n",
    "    by_type = by_type.rename(columns={'lora_rank_count': 'n_instances'}).round(4)\n",
    "    # Keep n_instances as integer\n",
    "    by_type['n_instances'] = by_type['n_instances'].astype(int)\n",
    "    display(by_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e16a3",
   "metadata": {},
   "source": [
    "## Statistical Test: Fulltext vs Generated Only (Rank 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTICAL TESTS: FULLTEXT vs GENERATED ONLY (Rank 64)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter for rank 64 only\n",
    "df_r64 = df[df['lora_rank'] == 64]\n",
    "\n",
    "# Get F1 scores for each model type\n",
    "fulltext_f1 = df_r64[df_r64['model_type'] == 'fulltext']['f1'].values\n",
    "generated_f1 = df_r64[df_r64['model_type'] == 'generated only']['f1'].values\n",
    "\n",
    "print(f\"\\nSample sizes: fulltext n={len(fulltext_f1)}, generated only n={len(generated_f1)}\")\n",
    "print(f\"Fulltext F1 scores: {fulltext_f1}\")\n",
    "print(f\"Generated only F1 scores: {generated_f1}\")\n",
    "print(f\"\\nFulltext mean: {fulltext_f1.mean():.4f} ± {fulltext_f1.std():.4f}\")\n",
    "print(f\"Generated only mean: {generated_f1.mean():.4f} ± {generated_f1.std():.4f}\")\n",
    "print(f\"Difference: {generated_f1.mean() - fulltext_f1.mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPOTHESIS TEST SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print(\"H0: F1_generated ≤ F1_fulltext  (generated is not better)\")\n",
    "print(\"H1: F1_generated > F1_fulltext  (generated IS better)\")\n",
    "print(\"This is a ONE-TAILED test.\")\n",
    "\n",
    "# 1. Independent samples t-test (ONE-TAILED)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"1. INDEPENDENT SAMPLES T-TEST (one-tailed)\")\n",
    "print(\"-\" * 80)\n",
    "t_stat, p_value_two = stats.ttest_ind(generated_f1, fulltext_f1)\n",
    "# One-tailed test (is generated > fulltext?)\n",
    "p_value_one = p_value_two / 2 if t_stat > 0 else 1 - (p_value_two / 2)\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value (one-tailed, H1: generated > fulltext): {p_value_one:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(fulltext_f1) + len(generated_f1) - 2}\")\n",
    "print(f\"Significant at α=0.05? {'Yes - generated IS significantly better' if p_value_one < 0.05 else 'No - cannot reject H0'}\")\n",
    "\n",
    "# 2. Mann-Whitney U test (non-parametric, ONE-TAILED)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2. MANN-WHITNEY U TEST (non-parametric, one-tailed)\")\n",
    "print(\"-\" * 80)\n",
    "u_stat_one, p_value_mw_one = stats.mannwhitneyu(generated_f1, fulltext_f1, alternative='greater')\n",
    "print(f\"U-statistic: {u_stat_one:.4f}\")\n",
    "print(f\"p-value (one-tailed, H1: generated > fulltext): {p_value_mw_one:.4f}\")\n",
    "print(f\"Significant at α=0.05? {'Yes - generated IS significantly better' if p_value_mw_one < 0.05 else 'No - cannot reject H0'}\")\n",
    "\n",
    "# 3. Permutation test (ONE-TAILED)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"3. PERMUTATION TEST (exact test, one-tailed)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def permutation_test(group1, group2, n_permutations=10000):\n",
    "    \"\"\"Permutation test for difference in means.\"\"\"\n",
    "    observed_diff = group2.mean() - group1.mean()\n",
    "    combined = np.concatenate([group1, group2])\n",
    "    n1 = len(group1)\n",
    "    \n",
    "    count = 0\n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        perm_diff = combined[n1:].mean() - combined[:n1].mean()\n",
    "        if perm_diff >= observed_diff:\n",
    "            count += 1\n",
    "    \n",
    "    return count / n_permutations\n",
    "\n",
    "p_value_perm = permutation_test(fulltext_f1, generated_f1, n_permutations=10000)\n",
    "print(f\"Observed difference: {generated_f1.mean() - fulltext_f1.mean():.4f}\")\n",
    "print(f\"p-value (one-tailed, 10,000 permutations): {p_value_perm:.4f}\")\n",
    "print(f\"Significant at α=0.05? {'Yes - generated IS significantly better' if p_value_perm < 0.05 else 'No - cannot reject H0'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nWith n={len(fulltext_f1)} observations per group:\")\n",
    "print(f\"The difference of {generated_f1.mean() - fulltext_f1.mean():.4f} in F1 score\")\n",
    "print(f\"t-test p-value: {p_value_one:.4f}\")\n",
    "print(f\"Mann-Whitney p-value: {p_value_mw_one:.4f}\")\n",
    "print(f\"Permutation test p-value: {p_value_perm:.4f}\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "if p_value_one < 0.05 or p_value_mw_one < 0.05 or p_value_perm < 0.05:\n",
    "    print(\"CONCLUSION: At least one test suggests generated IS significantly better (p < 0.05)\")\n",
    "else:\n",
    "    print(\"CONCLUSION: No significant evidence that generated is better than fulltext\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nNote: Very small sample sizes (n=3) limit statistical power.\")\n",
    "print(\"Results should be interpreted cautiously and preferably with more data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070644b5",
   "metadata": {},
   "source": [
    "## Visualization 2: Heatmap of F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization, treat 'submission' as pass 3\n",
    "df_viz = df.copy()\n",
    "df_viz['pass_viz'] = df_viz['pass'].apply(lambda x: 3 if x == 'submission' else x)\n",
    "\n",
    "# Create separate heatmaps for fulltext and generated only\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7.0, 2.8))  # Two-column width for ACL\n",
    "\n",
    "for idx, (model_type, ax) in enumerate(zip(['fulltext', 'generated only'], axes)):\n",
    "    df_subset = df_viz[df_viz['model_type'] == model_type]\n",
    "    pivot_f1 = df_subset.pivot_table(values='f1', index='lora_rank', columns='pass_viz', aggfunc='mean')\n",
    "    \n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.4f', cmap='YlGnBu', \n",
    "                cbar_kws={'label': 'F1 Score'}, ax=ax, linewidths=1.0,\n",
    "                annot_kws={'fontsize': 8}, vmin=0.57, vmax=0.64)\n",
    "    ax.set_xlabel('Pass Number', fontsize=11)\n",
    "    ax.set_ylabel('LoRA Rank', fontsize=11)\n",
    "    ax.set_title(f'F1 Score: {model_type.title()}', fontsize=12)\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'f1_heatmap_by_model_type.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(OUTPUT_DIR / 'f1_heatmap_by_model_type.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved to {OUTPUT_DIR / 'f1_heatmap_by_model_type.pdf'}\")\n",
    "print(\"Note: Submission files treated as pass 3 in visualizations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63590ad4",
   "metadata": {},
   "source": [
    "## Visualization 3: Precision-Recall Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f702e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 3.0))  # Single column width for ACL\n",
    "\n",
    "# For visualization, treat 'submission' as pass 3\n",
    "df_viz = df.copy()\n",
    "df_viz['pass_viz'] = df_viz['pass'].apply(lambda x: 3 if x == 'submission' else x)\n",
    "\n",
    "# Define markers for model types\n",
    "markers = {'fulltext': 'o', 'generated only': '^'}  # circle for fulltext, triangle for generated only\n",
    "\n",
    "# Get unique LoRA ranks and create color palette\n",
    "lora_ranks = sorted(df_viz['lora_rank'].unique())\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.9, len(lora_ranks)))\n",
    "rank_colors = dict(zip(lora_ranks, colors))\n",
    "\n",
    "# Scatter plot: color by rank, marker by model type\n",
    "for rank in lora_ranks:\n",
    "    for model_type in ['fulltext', 'generated only']:\n",
    "        data = df_viz[(df_viz['lora_rank'] == rank) & (df_viz['model_type'] == model_type)]\n",
    "        if len(data) > 0:\n",
    "            ax.scatter(data['recall'], data['precision'], \n",
    "                      s=100, alpha=0.7, \n",
    "                      marker=markers[model_type], \n",
    "                      color=rank_colors[rank], \n",
    "                      edgecolors='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=11)\n",
    "ax.set_ylabel('Precision', fontsize=11)\n",
    "ax.set_title('Precision-Recall Trade-off', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, linewidth=0.8)\n",
    "ax.set_xlim([0.48, 0.62])\n",
    "ax.set_ylim([0.62, 0.68])\n",
    "ax.tick_params(labelsize=10)\n",
    "\n",
    "# Create two-part legend\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Part 1: Colors for LoRA ranks\n",
    "rank_legend = [Patch(facecolor=rank_colors[rank], edgecolor='black', label=f'Rank {rank}') \n",
    "               for rank in lora_ranks]\n",
    "\n",
    "# Part 2: Markers for model types\n",
    "marker_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "                        markeredgecolor='black', markersize=8, label='Fulltext'),\n",
    "                Line2D([0], [0], marker='^', color='w', markerfacecolor='gray', \n",
    "                        markeredgecolor='black', markersize=8, label='Generated only')]\n",
    "\n",
    "# Combine legends\n",
    "all_handles = rank_legend + marker_legend\n",
    "all_labels = [h.get_label() for h in all_handles]\n",
    "\n",
    "# Add separator in legend (empty space)\n",
    "ax.legend(handles=all_handles, labels=all_labels, \n",
    "         frameon=True, loc='best', fontsize=7, title_fontsize=8,\n",
    "         ncol=1, columnspacing=1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'precision_recall_tradeoff.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(OUTPUT_DIR / 'precision_recall_tradeoff.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved to {OUTPUT_DIR / 'precision_recall_tradeoff.pdf'}\")\n",
    "print(\"Note: Colors = LoRA ranks, Markers = model types (circles=fulltext, triangles=generated only)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101241cd",
   "metadata": {},
   "source": [
    "## Visualization 4: Box Plots of Metrics by LoRA Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization, treat 'submission' as pass 3\n",
    "df_viz = df.copy()\n",
    "df_viz['pass_viz'] = df_viz['pass'].apply(lambda x: 3 if x == 'submission' else x)\n",
    "\n",
    "# Create separate plots for each metric\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "# Get unique LoRA ranks for separator lines\n",
    "lora_ranks = sorted(df_viz['lora_rank'].unique())\n",
    "\n",
    "for metric, label in zip(metrics, metric_labels):\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 2.5))\n",
    "    \n",
    "    # Use hue to differentiate model types with more distinct colors\n",
    "    bp = sns.boxplot(data=df_viz, x='lora_rank', y=metric, hue='model_type', ax=ax, \n",
    "                palette={'fulltext': '#2ca02c', 'generated only': '#d62728'}, \n",
    "                linewidth=2.0, width=1, gap=0.5)  \n",
    "    \n",
    "    # Add vertical separator lines between ranks\n",
    "    for i in range(len(lora_ranks) - 1):\n",
    "        ax.axvline(x=i + 0.5, color='gray', linestyle='--', linewidth=1.0, alpha=0.5)\n",
    "    \n",
    "    # Add thin separator lines between F and G boxes within each rank\n",
    "    for i in range(len(lora_ranks)):\n",
    "        ax.axvline(x=i, color='lightgray', linestyle='-', linewidth=0.5, alpha=0.4)\n",
    "    \n",
    "    # Add F/G labels on boxes using actual box positions\n",
    "    # Get the box patches from the plot\n",
    "    box_patches = [patch for patch in ax.patches if isinstance(patch, plt.matplotlib.patches.PathPatch)]\n",
    "    \n",
    "    # Seaborn creates boxes in order: all fulltext boxes, then all generated only boxes\n",
    "    n_ranks = len(lora_ranks)\n",
    "    \n",
    "    # Determine y position based on the actual data range\n",
    "    y_min = df_viz[metric].min()\n",
    "    y_max = df_viz[metric].max()\n",
    "    y_pos = y_min + (y_max - y_min) * 1.1  # Position at 15% from bottom\n",
    "    \n",
    "    for i, rank in enumerate(lora_ranks):\n",
    "        # Fulltext box is at index i, generated only box is at index i + n_ranks\n",
    "        if i < len(box_patches):\n",
    "            # Fulltext box\n",
    "            genonly_box = box_patches[i]\n",
    "            x_pos = genonly_box.get_path().vertices[:, 0].mean()\n",
    "            ax.text(x_pos, y_pos, 'G', fontsize=8, ha='center', va='center',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black', linewidth=0.5, alpha=0.5))\n",
    "        \n",
    "        if i + n_ranks < len(box_patches):\n",
    "            # Generated only box\n",
    "            fulltext_box = box_patches[i + n_ranks]\n",
    "            x_pos = fulltext_box.get_path().vertices[:, 0].mean()\n",
    "            ax.text(x_pos, y_pos, 'F', fontsize=8, ha='center', va='center',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black', linewidth=0.5, alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('LoRA Rank', fontsize=11)\n",
    "    ax.set_ylabel(label, fontsize=11)\n",
    "    ax.set_title(f'{label} by LoRA Rank', fontsize=12)\n",
    "    ax.legend(frameon=True, loc='lower right', fontsize=9, title_fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis='y', linewidth=0.8)\n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Set y-axis limits based on data range with 10% padding\n",
    "    y_min = df_viz[metric].min()\n",
    "    y_max = df_viz[metric].max()\n",
    "    y_range = y_max - y_min\n",
    "    padding = y_range * 0.15  # 15% padding\n",
    "    ax.set_ylim([y_min - padding, y_max + padding])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / f'{metric}_boxplot_by_lora_rank.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(OUTPUT_DIR / f'{metric}_boxplot_by_lora_rank.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved to {OUTPUT_DIR / f'{metric}_boxplot_by_lora_rank.pdf'}\")\n",
    "\n",
    "print(\"\\nAll individual metric plots saved\")\n",
    "print(\"Note: F = Fulltext, G = Generated only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3efe20",
   "metadata": {},
   "source": [
    "## Visualization 5: Comparison of All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6590c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization, treat 'submission' as pass 3\n",
    "df_viz = df.copy()\n",
    "df_viz['pass_viz'] = df_viz['pass'].apply(lambda x: 3 if x == 'submission' else x)\n",
    "\n",
    "# Average metrics across all passes for each LoRA rank and model type\n",
    "avg_metrics = df_viz.groupby(['lora_rank', 'model_type'])[['accuracy', 'precision', 'recall', 'f1']].mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.0, 3.5))  # Two-column width for ACL\n",
    "\n",
    "# Get unique LoRA ranks\n",
    "lora_ranks = sorted(avg_metrics['lora_rank'].unique())\n",
    "x = np.arange(len(lora_ranks))\n",
    "width = 0.09  # Width of bars (4 metrics * 2 model types = 8 bars per rank)\n",
    "\n",
    "# Define colors and patterns for model types\n",
    "colors = ['#377eb8', '#4daf4a', '#e41a1c', '#ff7f00']  # Blue, Green, Red, Orange\n",
    "hatches = {'fulltext': '', 'generated only': '///'}  # Solid for fulltext, hatched for generated only\n",
    "\n",
    "for metric_idx, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
    "    for model_idx, model_type in enumerate(['fulltext', 'generated only']):\n",
    "        data = avg_metrics[avg_metrics['model_type'] == model_type]\n",
    "        offset = (metric_idx * 2 + model_idx - 3.5) * width\n",
    "        \n",
    "        bars = ax.bar(x + offset, data[metric].values, width, \n",
    "                     label=f'{metric.capitalize()} ({model_type[:4]})' if metric_idx == 0 else '',\n",
    "                     alpha=0.8, linewidth=1.0, edgecolor='black',\n",
    "                     color=colors[metric_idx], hatch=hatches[model_type])\n",
    "\n",
    "ax.set_xlabel('LoRA Rank', fontsize=11)\n",
    "ax.set_ylabel('Score', fontsize=11)\n",
    "ax.set_title('Average Performance Metrics by LoRA Rank and Model Type', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'r={r}' for r in lora_ranks], fontsize=10)\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=colors[0], edgecolor='black', label='Accuracy'),\n",
    "    Patch(facecolor=colors[1], edgecolor='black', label='Precision'),\n",
    "    Patch(facecolor=colors[2], edgecolor='black', label='Recall'),\n",
    "    Patch(facecolor=colors[3], edgecolor='black', label='F1'),\n",
    "    Patch(facecolor='gray', edgecolor='black', label='Fulltext'),\n",
    "    Patch(facecolor='gray', edgecolor='black', hatch='///', label='Generated only')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='best', frameon=True, fontsize=9, ncol=2)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y', linewidth=0.8)\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'metrics_comparison_bar.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(OUTPUT_DIR / 'metrics_comparison_bar.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved to {OUTPUT_DIR / 'metrics_comparison_bar.pdf'}\")\n",
    "print(\"Note: Solid bars = fulltext, Hatched bars = generated only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72328a72",
   "metadata": {},
   "source": [
    "## Visualization 6: Confusion Matrix Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization, treat 'submission' as pass 3\n",
    "df_viz = df.copy()\n",
    "df_viz['pass_viz'] = df_viz['pass'].apply(lambda x: 3 if x == 'submission' else x)\n",
    "\n",
    "# Average confusion matrix values by LoRA rank and model type\n",
    "avg_confusion = df_viz.groupby(['lora_rank', 'model_type'])[['TP', 'TN', 'FP', 'FN']].mean().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.0, 3.5))  # Two-column width for ACL\n",
    "\n",
    "# Get unique LoRA ranks\n",
    "lora_ranks = sorted(avg_confusion['lora_rank'].unique())\n",
    "x = np.arange(len(lora_ranks))\n",
    "width = 0.09  # Width of bars (4 components * 2 model types = 8 bars per rank)\n",
    "\n",
    "# Define colors and patterns\n",
    "colors = ['green', 'lightgreen', 'orange', 'red']\n",
    "hatches = {'fulltext': '', 'generated only': '///'}\n",
    "\n",
    "for comp_idx, component in enumerate(['TP', 'TN', 'FP', 'FN']):\n",
    "    for model_idx, model_type in enumerate(['fulltext', 'generated only']):\n",
    "        data = avg_confusion[avg_confusion['model_type'] == model_type]\n",
    "        offset = (comp_idx * 2 + model_idx - 3.5) * width\n",
    "        \n",
    "        bars = ax.bar(x + offset, data[component].values, width, \n",
    "                     alpha=0.8, linewidth=1.0, edgecolor='black',\n",
    "                     color=colors[comp_idx], hatch=hatches[model_type])\n",
    "\n",
    "ax.set_xlabel('LoRA Rank', fontsize=11)\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "ax.set_title('Confusion Matrix Components by LoRA Rank and Model Type', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'r={r}' for r in lora_ranks], fontsize=10)\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', edgecolor='black', label='True Positive'),\n",
    "    Patch(facecolor='lightgreen', edgecolor='black', label='True Negative'),\n",
    "    Patch(facecolor='orange', edgecolor='black', label='False Positive'),\n",
    "    Patch(facecolor='red', edgecolor='black', label='False Negative'),\n",
    "    Patch(facecolor='gray', edgecolor='black', label='Fulltext'),\n",
    "    Patch(facecolor='gray', edgecolor='black', hatch='///', label='Generated only')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='best', frameon=True, fontsize=9, ncol=2)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y', linewidth=0.8)\n",
    "ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix_components.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix_components.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved to {OUTPUT_DIR / 'confusion_matrix_components.pdf'}\")\n",
    "print(\"Note: Solid bars = fulltext, Hatched bars = generated only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf869b",
   "metadata": {},
   "source": [
    "## Statistical Analysis: Variance Across Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance in F1 across passes for each LoRA rank\n",
    "variance_analysis = df.groupby('lora_rank')['f1'].agg(['mean', 'std', 'min', 'max'])\n",
    "variance_analysis['range'] = variance_analysis['max'] - variance_analysis['min']\n",
    "variance_analysis = variance_analysis.round(4)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VARIANCE ANALYSIS: F1 Score Stability Across Passes\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nLower standard deviation indicates more consistent performance across passes.\")\n",
    "display(variance_analysis)\n",
    "\n",
    "# Find most stable model\n",
    "most_stable = variance_analysis['std'].idxmin()\n",
    "print(f\"\\nMost stable model (lowest std): LoRA rank {most_stable} (std={variance_analysis.loc[most_stable, 'std']:.4f})\")\n",
    "\n",
    "# Find best performing model\n",
    "best_performing = variance_analysis['mean'].idxmax()\n",
    "print(f\"Best performing model (highest mean F1): LoRA rank {best_performing} (mean={variance_analysis.loc[best_performing, 'mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c6bef",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal models evaluated: {len(df)}\")\n",
    "print(f\"LoRA ranks tested: {sorted(df['lora_rank'].unique())}\")\n",
    "print(f\"Inference passes: {sorted(df['pass'].unique(), key=str)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"BEST OVERALL MODEL\")\n",
    "print(\"-\" * 80)\n",
    "best_model = df.loc[df['f1'].idxmax()]\n",
    "print(f\"Configuration: LoRA rank {best_model['lora_rank']}, Pass {best_model['pass']}\")\n",
    "print(f\"F1 Score: {best_model['f1']:.4f}\")\n",
    "print(f\"Precision: {best_model['precision']:.4f}\")\n",
    "print(f\"Recall: {best_model['recall']:.4f}\")\n",
    "print(f\"Accuracy: {best_model['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Trend analysis (only for numeric passes)\n",
    "df_numeric_passes = df[df['pass'].apply(lambda x: isinstance(x, int))].copy()\n",
    "if len(df_numeric_passes) > 0:\n",
    "    pass_improvement = df_numeric_passes.groupby('pass')['f1'].mean().sort_index()\n",
    "    if len(pass_improvement) > 1:\n",
    "        if pass_improvement.is_monotonic_increasing:\n",
    "            print(\"Performance improves consistently with more passes\")\n",
    "        elif pass_improvement.is_monotonic_decreasing:\n",
    "            print(\"Performance degrades with more passes\")\n",
    "        else:\n",
    "            print(\"Performance varies across passes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
