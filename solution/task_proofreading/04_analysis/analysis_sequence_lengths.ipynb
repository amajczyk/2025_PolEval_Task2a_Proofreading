{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fdbdc",
   "metadata": {},
   "source": [
    "## Load System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Polish system prompt\n",
    "with open('system_prompts/proofreading/system_prompt_pl_proofreading', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT = f.read().strip()\n",
    "\n",
    "print(f\"System prompt length: {len(SYSTEM_PROMPT)} characters\")\n",
    "print(f\"System prompt (first 200 chars):\\n{SYSTEM_PROMPT[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcdbd7",
   "metadata": {},
   "source": [
    "## Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load all datasets\n",
    "train_data = load_jsonl('data/taskA/train.jsonl')\n",
    "dev_data = load_jsonl('data/taskA/dev.jsonl')\n",
    "test_a_data = load_jsonl('data/taskA/test_A.jsonl')\n",
    "test_b_data = load_jsonl('data/taskA/test_gold_standard_normalised_B.jsonl')\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "print(f\"Loaded {len(dev_data)} validation examples\")\n",
    "print(f\"Loaded {len(test_a_data)} test A examples\")\n",
    "print(f\"Loaded {len(test_b_data)} test B examples (with targets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f23c8a",
   "metadata": {},
   "source": [
    "## Calculate Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lengths(data, system_prompt, has_target=True):\n",
    "    \"\"\"Calculate various length statistics for the dataset.\"\"\"\n",
    "    source_lengths = []\n",
    "    target_lengths = []\n",
    "    prompt_lengths = []\n",
    "    system_plus_prompt_lengths = []\n",
    "    total_input_lengths = []\n",
    "    \n",
    "    system_prompt_len = len(system_prompt)\n",
    "    \n",
    "    for item in data:\n",
    "        source_len = len(item['source'])\n",
    "        prompt_len = len(item['prompt'])\n",
    "        \n",
    "        source_lengths.append(source_len)\n",
    "        prompt_lengths.append(prompt_len)\n",
    "        \n",
    "        if has_target and item.get('target') is not None:\n",
    "            target_len = len(item['target'])\n",
    "            target_lengths.append(target_len)\n",
    "        \n",
    "        system_plus_prompt = system_prompt_len + prompt_len\n",
    "        system_plus_prompt_lengths.append(system_plus_prompt)\n",
    "        \n",
    "        total_input = system_prompt_len + prompt_len + source_len\n",
    "        total_input_lengths.append(total_input)\n",
    "    \n",
    "    result = {\n",
    "        'source': source_lengths,\n",
    "        'prompt': prompt_lengths,\n",
    "        'system_plus_prompt': system_plus_prompt_lengths,\n",
    "        'total_input': total_input_lengths\n",
    "    }\n",
    "    \n",
    "    if target_lengths:\n",
    "        result['target'] = target_lengths\n",
    "    \n",
    "    return result\n",
    "\n",
    "train_lengths = calculate_lengths(train_data, SYSTEM_PROMPT, has_target=True)\n",
    "dev_lengths = calculate_lengths(dev_data, SYSTEM_PROMPT, has_target=True)\n",
    "test_a_lengths = calculate_lengths(test_a_data, SYSTEM_PROMPT, has_target=False)\n",
    "test_b_lengths = calculate_lengths(test_b_data, SYSTEM_PROMPT, has_target=True)\n",
    "\n",
    "print(\"Length calculations completed for all datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b15fb",
   "metadata": {},
   "source": [
    "## Statistics - Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(lengths, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Print detailed statistics for length measurements.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{dataset_name.upper()} - SEQUENCE LENGTH STATISTICS (in characters)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stats = [\n",
    "        (\"Source text\", lengths['source']),\n",
    "        (\"Target text\", lengths['target']),\n",
    "        (\"Task prompt\", lengths['prompt']),\n",
    "        (\"System prompt + Task prompt\", lengths['system_plus_prompt']),\n",
    "        (\"Total input (System + Task prompt + Source)\", lengths['total_input'])\n",
    "    ]\n",
    "    \n",
    "    for name, values in stats:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean:     {np.mean(values):8.2f}\")\n",
    "        print(f\"  Median:   {np.median(values):8.2f}\")\n",
    "        print(f\"  Std Dev:  {np.std(values):8.2f}\")\n",
    "        print(f\"  Min:      {np.min(values):8.0f}\")\n",
    "        print(f\"  Max:      {np.max(values):8.0f}\")\n",
    "        print(f\"  95th %:   {np.percentile(values, 95):8.2f}\")\n",
    "        print(f\"  99th %:   {np.percentile(values, 99):8.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Average length increase (Target - Source): {np.mean(lengths['target']) - np.mean(lengths['source']):.2f} chars\")\n",
    "    print(f\"Relative increase: {(np.mean(lengths['target']) / np.mean(lengths['source']) - 1) * 100:.2f}%\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print_statistics(train_lengths, \"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c43f9",
   "metadata": {},
   "source": [
    "## Statistics - Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(dev_lengths, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055842e",
   "metadata": {},
   "source": [
    "## Statistics - Test A Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52155f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_statistics(lengths, dataset_name=\"Test Set\"):\n",
    "    \"\"\"Print statistics for test data (no targets available).\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{dataset_name.upper()} - SOURCE LENGTH STATISTICS (in characters)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nSource text:\")\n",
    "    print(f\"  Mean:     {np.mean(lengths['source']):8.2f}\")\n",
    "    print(f\"  Median:   {np.median(lengths['source']):8.2f}\")\n",
    "    print(f\"  Std Dev:  {np.std(lengths['source']):8.2f}\")\n",
    "    print(f\"  Min:      {np.min(lengths['source']):8.0f}\")\n",
    "    print(f\"  Max:      {np.max(lengths['source']):8.0f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print_test_statistics(test_a_lengths, \"Test A Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303c4ea",
   "metadata": {},
   "source": [
    "## Statistics - Test B Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(test_b_lengths, \"Test B Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c6a24",
   "metadata": {},
   "source": [
    "## Token Estimation\n",
    "\n",
    "Estimate token counts (rough approximation: 1 token â‰ˆ 4 characters for English, but Polish may differ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccbda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough token estimation (characters / 4)\n",
    "CHARS_PER_TOKEN = 4  # This is an approximation\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTIMATED TOKEN COUNTS (rough approximation)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAssuming ~{CHARS_PER_TOKEN} characters per token (adjust based on actual tokenizer)\\n\")\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(f\"  Average source tokens:              ~{np.mean(train_lengths['source']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Average target tokens:              ~{np.mean(train_lengths['target']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Average system+prompt tokens:       ~{np.mean(train_lengths['system_plus_prompt']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Average total input tokens:         ~{np.mean(train_lengths['total_input']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Max total input tokens:             ~{np.max(train_lengths['total_input']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  99th percentile total input tokens: ~{np.percentile(train_lengths['total_input'], 99) / CHARS_PER_TOKEN:.0f}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  Average source tokens:              ~{np.mean(dev_lengths['source']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Average target tokens:              ~{np.mean(dev_lengths['target']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Average system+prompt tokens:       ~{np.mean(dev_lengths['system_plus_prompt']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Average total input tokens:         ~{np.mean(dev_lengths['total_input']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(f\"  Max total input tokens:             ~{np.max(dev_lengths['total_input']) / CHARS_PER_TOKEN:.0f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8810026",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Sequence Length Distributions - Training Set', fontsize=16)\n",
    "\n",
    "axes[0, 0].hist(train_lengths['source'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(np.mean(train_lengths['source']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_lengths['source']):.0f}\")\n",
    "axes[0, 0].set_xlabel('Length (characters)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Source Text Length')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(train_lengths['target'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].axvline(np.mean(train_lengths['target']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_lengths['target']):.0f}\")\n",
    "axes[0, 1].set_xlabel('Length (characters)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Target Text Length')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(train_lengths['system_plus_prompt'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 0].axvline(np.mean(train_lengths['system_plus_prompt']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_lengths['system_plus_prompt']):.0f}\")\n",
    "axes[1, 0].set_xlabel('Length (characters)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('System Prompt + Task Prompt Length')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(train_lengths['total_input'], bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].axvline(np.mean(train_lengths['total_input']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_lengths['total_input']):.0f}\")\n",
    "axes[1, 1].axvline(4096 * CHARS_PER_TOKEN, color='orange', linestyle='--', label='4096 tokens (~16384 chars)')\n",
    "axes[1, 1].set_xlabel('Length (characters)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Total Input Length (System + Task Prompt + Source)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequence_length_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved as 'sequence_length_distributions.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69629a87",
   "metadata": {},
   "source": [
    "## Check Examples That Exceed Token Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d32e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many examples exceed common token limits\n",
    "token_limits = [512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLES EXCEEDING TOKEN LIMITS (Training Set)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for limit in token_limits:\n",
    "    char_limit = limit * CHARS_PER_TOKEN\n",
    "    exceeding = sum(1 for length in train_lengths['total_input'] if length > char_limit)\n",
    "    percentage = (exceeding / len(train_lengths['total_input'])) * 100\n",
    "    print(f\"\\nToken limit: {limit:5d} (~{char_limit:6d} chars)\")\n",
    "    print(f\"  Exceeding examples: {exceeding:5d} / {len(train_lengths['total_input'])} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec3ce5",
   "metadata": {},
   "source": [
    "## Summary for Paper\n",
    "\n",
    "Key statistics to report in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY FOR PAPER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset size:\")\n",
    "print(f\"  Training examples:   {len(train_data):,}\")\n",
    "print(f\"  Validation examples: {len(dev_data):,}\")\n",
    "\n",
    "print(f\"\\nSystem prompt:\")\n",
    "print(f\"  Length: {len(SYSTEM_PROMPT)} characters (~{len(SYSTEM_PROMPT) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "\n",
    "print(f\"\\nAverage task prompt:\")\n",
    "print(f\"  Length: {np.mean(train_lengths['prompt']):.2f} characters (~{np.mean(train_lengths['prompt']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "\n",
    "print(f\"\\nSource texts (to be transformed):\")\n",
    "print(f\"  Mean length:   {np.mean(train_lengths['source']):.2f} characters (~{np.mean(train_lengths['source']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "print(f\"  Median length: {np.median(train_lengths['source']):.2f} characters (~{np.median(train_lengths['source']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "print(f\"  Max length:    {np.max(train_lengths['source'])} characters (~{np.max(train_lengths['source']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "\n",
    "print(f\"\\nTarget texts (gender-inclusive):\")\n",
    "print(f\"  Mean length:   {np.mean(train_lengths['target']):.2f} characters (~{np.mean(train_lengths['target']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "print(f\"  Average increase: {np.mean(train_lengths['target']) - np.mean(train_lengths['source']):.2f} chars ({(np.mean(train_lengths['target']) / np.mean(train_lengths['source']) - 1) * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal input length (System + Task prompt + Source):\")\n",
    "print(f\"  Mean:   {np.mean(train_lengths['total_input']):.2f} characters (~{np.mean(train_lengths['total_input']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "print(f\"  Median: {np.median(train_lengths['total_input']):.2f} characters (~{np.median(train_lengths['total_input']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "print(f\"  95th percentile: {np.percentile(train_lengths['total_input'], 95):.2f} characters (~{np.percentile(train_lengths['total_input'], 95) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "print(f\"  Max:    {np.max(train_lengths['total_input'])} characters (~{np.max(train_lengths['total_input']) / CHARS_PER_TOKEN:.0f} tokens)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Note: Token estimates assume ~4 characters per token.\")\n",
    "print(\"For accurate token counts, use the actual tokenizer (e.g., Qwen3 tokenizer).\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c633a94",
   "metadata": {},
   "source": [
    "## Generate Comprehensive LaTeX Table for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e849e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\begin{table}[!htbp]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\small\")\n",
    "print(\"\\\\caption{Dataset statistics: character-level sequence lengths}\")\n",
    "print(\"\\\\label{tab:dataset_stats}\")\n",
    "print()\n",
    "print(\"\\\\begin{tabularx}{\\\\columnwidth}{Xrrrr}\")\n",
    "print(\"\\\\toprule\")\n",
    "print(\"\\\\textbf{Metric} & \\\\textbf{Train} & \\\\textbf{Val} & \\\\textbf{Test A} & \\\\textbf{Test B} \\\\\\\\\")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "# Source text\n",
    "print(\"\\\\multicolumn{5}{l}{\\\\textbf{\\\\textit{Source text}}} \\\\\\\\\")\n",
    "print(f\"Mean & {np.mean(train_lengths['source']):.2f} & {np.mean(dev_lengths['source']):.2f} & {np.mean(test_a_lengths['source']):.2f} & {np.mean(test_b_lengths['source']):.2f} \\\\\\\\\")\n",
    "print(f\"Std Dev & {np.std(train_lengths['source']):.2f} & {np.std(dev_lengths['source']):.2f} & {np.std(test_a_lengths['source']):.2f} & {np.std(test_b_lengths['source']):.2f} \\\\\\\\\")\n",
    "print(f\"Median & {np.median(train_lengths['source']):.2f} & {np.median(dev_lengths['source']):.2f} & {np.median(test_a_lengths['source']):.2f} & {np.median(test_b_lengths['source']):.2f} \\\\\\\\\")\n",
    "print(f\"Min & {np.min(train_lengths['source']):.2f} & {np.min(dev_lengths['source']):.2f} & {np.min(test_a_lengths['source']):.2f} & {np.min(test_b_lengths['source']):.2f} \\\\\\\\\")\n",
    "print(f\"Max & {np.max(train_lengths['source']):.2f} & {np.max(dev_lengths['source']):.2f} & {np.max(test_a_lengths['source']):.2f} & {np.max(test_b_lengths['source']):.2f} \\\\\\\\\")\n",
    "print(f\"95th \\\\% & {np.percentile(train_lengths['source'], 95):.2f} & {np.percentile(dev_lengths['source'], 95):.2f} & {np.percentile(test_a_lengths['source'], 95):.2f} & {np.percentile(test_b_lengths['source'], 95):.2f} \\\\\\\\\")\n",
    "print(f\"99th \\\\% & {np.percentile(train_lengths['source'], 99):.2f} & {np.percentile(dev_lengths['source'], 99):.2f} & {np.percentile(test_a_lengths['source'], 99):.2f} & {np.percentile(test_b_lengths['source'], 99):.2f} \\\\\\\\\")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "# Target text\n",
    "print(\"\\\\multicolumn{5}{l}{\\\\textbf{\\\\textit{Target text}}\\\\textsuperscript{2}} \\\\\\\\\")\n",
    "print(f\"Mean & {np.mean(train_lengths['target']):.2f} & {np.mean(dev_lengths['target']):.2f} & --- & {np.mean(test_b_lengths['target']):.2f} \\\\\\\\\")\n",
    "print(f\"Std Dev & {np.std(train_lengths['target']):.2f} & {np.std(dev_lengths['target']):.2f} & --- & {np.std(test_b_lengths['target']):.2f} \\\\\\\\\")\n",
    "print(f\"Median & {np.median(train_lengths['target']):.2f} & {np.median(dev_lengths['target']):.2f} & --- & {np.median(test_b_lengths['target']):.2f} \\\\\\\\\")\n",
    "print(f\"Min & {np.min(train_lengths['target']):.2f} & {np.min(dev_lengths['target']):.2f} & --- & {np.min(test_b_lengths['target']):.2f} \\\\\\\\\")\n",
    "print(f\"Max & {np.max(train_lengths['target']):.2f} & {np.max(dev_lengths['target']):.2f} & --- & {np.max(test_b_lengths['target']):.2f} \\\\\\\\\")\n",
    "print(f\"95th \\\\% & {np.percentile(train_lengths['target'], 95):.2f} & {np.percentile(dev_lengths['target'], 95):.2f} & --- & {np.percentile(test_b_lengths['target'], 95):.2f} \\\\\\\\\")\n",
    "print(f\"99th \\\\% & {np.percentile(train_lengths['target'], 99):.2f} & {np.percentile(dev_lengths['target'], 99):.2f} & --- & {np.percentile(test_b_lengths['target'], 99):.2f} \\\\\\\\\")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "# Task prompt\n",
    "print(\"\\\\multicolumn{5}{l}{\\\\textbf{\\\\textit{Task prompt}}} \\\\\\\\\")\n",
    "print(f\"Mean & {np.mean(train_lengths['prompt']):.2f} & {np.mean(dev_lengths['prompt']):.2f} & {np.mean(test_a_lengths['prompt']):.2f} & {np.mean(test_b_lengths['prompt']):.2f} \\\\\\\\\")\n",
    "print(f\"Std Dev & {np.std(train_lengths['prompt']):.2f} & {np.std(dev_lengths['prompt']):.2f} & {np.std(test_a_lengths['prompt']):.2f} & {np.std(test_b_lengths['prompt']):.2f} \\\\\\\\\")\n",
    "print(f\"Median & {np.median(train_lengths['prompt']):.2f} & {np.median(dev_lengths['prompt']):.2f} & {np.median(test_a_lengths['prompt']):.2f} & {np.median(test_b_lengths['prompt']):.2f} \\\\\\\\\")\n",
    "print(f\"Min & {np.min(train_lengths['prompt']):.2f} & {np.min(dev_lengths['prompt']):.2f} & {np.min(test_a_lengths['prompt']):.2f} & {np.min(test_b_lengths['prompt']):.2f} \\\\\\\\\")\n",
    "print(f\"Max & {np.max(train_lengths['prompt']):.2f} & {np.max(dev_lengths['prompt']):.2f} & {np.max(test_a_lengths['prompt']):.2f} & {np.max(test_b_lengths['prompt']):.2f} \\\\\\\\\")\n",
    "print(f\"95th \\\\% & {np.percentile(train_lengths['prompt'], 95):.2f} & {np.percentile(dev_lengths['prompt'], 95):.2f} & {np.percentile(test_a_lengths['prompt'], 95):.2f} & {np.percentile(test_b_lengths['prompt'], 95):.2f} \\\\\\\\\")\n",
    "print(f\"99th \\\\% & {np.percentile(train_lengths['prompt'], 99):.2f} & {np.percentile(dev_lengths['prompt'], 99):.2f} & {np.percentile(test_a_lengths['prompt'], 99):.2f} & {np.percentile(test_b_lengths['prompt'], 99):.2f} \\\\\\\\\")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "# Total input length\n",
    "print(\"% Using a footnote marker for the long definition to save space\")\n",
    "print(\"\\\\multicolumn{5}{l}{\\\\textbf{\\\\textit{Total input}}\\\\textsuperscript{1}} \\\\\\\\\")\n",
    "print(f\"Mean & {np.mean(train_lengths['total_input']):.2f} & {np.mean(dev_lengths['total_input']):.2f} & {np.mean(test_a_lengths['total_input']):.2f} & {np.mean(test_b_lengths['total_input']):.2f} \\\\\\\\\")\n",
    "print(f\"Std Dev & {np.std(train_lengths['total_input']):.2f} & {np.std(dev_lengths['total_input']):.2f} & {np.std(test_a_lengths['total_input']):.2f} & {np.std(test_b_lengths['total_input']):.2f} \\\\\\\\\")\n",
    "print(f\"Median & {np.median(train_lengths['total_input']):.2f} & {np.median(dev_lengths['total_input']):.2f} & {np.median(test_a_lengths['total_input']):.2f} & {np.median(test_b_lengths['total_input']):.2f} \\\\\\\\\")\n",
    "print(f\"Min & {np.min(train_lengths['total_input']):.2f} & {np.min(dev_lengths['total_input']):.2f} & {np.min(test_a_lengths['total_input']):.2f} & {np.min(test_b_lengths['total_input']):.2f} \\\\\\\\\")\n",
    "print(f\"Max & {np.max(train_lengths['total_input']):.2f} & {np.max(dev_lengths['total_input']):.2f} & {np.max(test_a_lengths['total_input']):.2f} & {np.max(test_b_lengths['total_input']):.2f} \\\\\\\\\")\n",
    "print(f\"95th \\\\% & {np.percentile(train_lengths['total_input'], 95):.2f} & {np.percentile(dev_lengths['total_input'], 95):.2f} & {np.percentile(test_a_lengths['total_input'], 95):.2f} & {np.percentile(test_b_lengths['total_input'], 95):.2f} \\\\\\\\\")\n",
    "print(f\"99th \\\\% & {np.percentile(train_lengths['total_input'], 99):.2f} & {np.percentile(dev_lengths['total_input'], 99):.2f} & {np.percentile(test_a_lengths['total_input'], 99):.2f} & {np.percentile(test_b_lengths['total_input'], 99):.2f} \\\\\\\\\")\n",
    "print(\"\\\\bottomrule\")\n",
    "\n",
    "print(\"\\\\end{tabularx}\")\n",
    "print()\n",
    "print(\"\\\\vspace{2pt}\")\n",
    "print(\"\\\\raggedright\")\n",
    "print(f\"\\\\footnotesize{{\\\\textsuperscript{{1}} System ({len(SYSTEM_PROMPT)} char.) + Task prompt + Source}}\")\n",
    "print()\n",
    "print(\"\\\\footnotesize{\\\\textsuperscript{2} Blank fields denote the fact that test sets were shared with no target sequences available to the participants at the time of the competition}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth-pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
