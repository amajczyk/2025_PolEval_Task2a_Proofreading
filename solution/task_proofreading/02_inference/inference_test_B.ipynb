{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463ab84e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d21673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration - adjust to match your trained model\n",
    "MODEL_SIZE = \"8B\"\n",
    "LORA_RANK = 64\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 10\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "\n",
    "# Path to your trained model - using the best checkpoint\n",
    "# Based on trainer_state.json, checkpoint-23000 had the best eval_loss\n",
    "MODEL_PATH = f\"../../../outputs/qwen3_{MODEL_SIZE}_polish_inclusive_proofreading_lora_r{LORA_RANK}_lr{LEARNING_RATE}_ep{EPOCHS}_bs{BATCH_SIZE}_ga{GRADIENT_ACCUMULATION_STEPS}_warmup{WARMUP_STEPS}_seq{MAX_SEQ_LENGTH}/checkpoint-23000\"\n",
    "MODEL_PATH = os.path.abspath(MODEL_PATH)\n",
    "print(f\"Using best model from: {MODEL_PATH}\")\n",
    "\n",
    "# Inference parameters\n",
    "TEMPERATURE = 0.3  # Lower temperature for more precise transformations\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "MAX_NEW_TOKENS = 4096  # Enough for longer texts\n",
    "\n",
    "# File paths\n",
    "TEST_FILE = \"../../../data/taskA/test_B.jsonl\"\n",
    "OUTPUT_FILE = \"predictions_test_B.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc0434",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix HuggingFace cache permissions\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/d/Pobrane/poleval-gender/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/d/Pobrane/poleval-gender/.cache/huggingface/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/d/Pobrane/poleval-gender/.cache/huggingface/datasets'\n",
    "os.environ['TRITON_CACHE_DIR'] = '/mnt/d/Pobrane/poleval-gender/.cache/triton'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d3785",
   "metadata": {},
   "source": [
    "### Load the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Enable inference mode for 2x faster generation\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448e741",
   "metadata": {},
   "source": [
    "### Load System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Polish system prompt used during training\n",
    "with open('../../../system_prompts/proofreading/system_prompt_pl_proofreading', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT = f.read().strip()\n",
    "\n",
    "print(\"System prompt loaded.\")\n",
    "print(f\"System prompt length: {len(SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffff057",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load test data\n",
    "test_data = load_jsonl(TEST_FILE)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test examples from {TEST_FILE}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"IPIS ID: {test_data[0]['ipis_id']}\")\n",
    "print(f\"Prompt: {test_data[0]['prompt'][:100]}...\")\n",
    "print(f\"Source: {test_data[0]['source'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9da15d",
   "metadata": {},
   "source": [
    "### Generate Predictions\n",
    "\n",
    "This will generate gender-inclusive versions for all texts in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96df53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = \"inference_checkpoints\"\n",
    "SAVE_INTERVAL = 25\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Check for existing checkpoints to resume from\n",
    "checkpoint_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"predictions_checkpoint_\")])\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "    print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Loading predictions from checkpoint...\")\n",
    "    \n",
    "    with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "    \n",
    "    processed_ids = {p['ipis_id'] for p in predictions}\n",
    "    start_idx = len(predictions)\n",
    "    print(f\"Resuming from example {start_idx} ({len(predictions)} already processed)\")\n",
    "else:\n",
    "    predictions = []\n",
    "    processed_ids = set()\n",
    "    start_idx = 0\n",
    "    print(\"Starting from scratch\")\n",
    "\n",
    "print(f\"\\nGenerating predictions for {len(test_data)} examples...\")\n",
    "print(f\"Parameters: temperature={TEMPERATURE}, top_p={TOP_P}, max_new_tokens={MAX_NEW_TOKENS}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    for idx, item in enumerate(tqdm(test_data[start_idx:], initial=start_idx, total=len(test_data), desc=\"Generating predictions\")):\n",
    "        # Skip if already processed\n",
    "        if item['ipis_id'] in processed_ids:\n",
    "            continue\n",
    "        \n",
    "        # Construct the prompt using the same format as training\n",
    "        user_message = item['prompt'] + item['source']\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and move to GPU\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                top_k=TOP_K,\n",
    "                do_sample=True if TEMPERATURE > 0 else False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated tokens (excluding the input prompt)\n",
    "        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up any remaining chat template artifacts\n",
    "        if \"<|im_end|>\" in response:\n",
    "            response = response.split(\"<|im_end|>\")[0]\n",
    "        if \"<|im_start|>\" in response:\n",
    "            response = response.split(\"<|im_start|>\")[-1]\n",
    "            if \"\\n\" in response:\n",
    "                response = response.split(\"\\n\", 1)[-1]\n",
    "        \n",
    "        # For debugging: also save the full decoded output\n",
    "        full_generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Create prediction entry in the same format as sample_proofreading.jsonl\n",
    "        prediction = {\n",
    "            \"ipis_id\": item['ipis_id'],\n",
    "            \"source\": item['source'],\n",
    "            \"target\": response.strip(),\n",
    "            \"normalised_target\": None,  # Will be filled in later normalization step\n",
    "            \"full_generated_text\": full_generated_text  # Save complete output for debugging\n",
    "        }\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        processed_ids.add(item['ipis_id'])\n",
    "        \n",
    "        # Save checkpoint every 10 examples\n",
    "        if len(predictions) % SAVE_INTERVAL == 0:\n",
    "            checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"predictions_checkpoint_{len(predictions):05d}.jsonl\")\n",
    "            with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                for pred in predictions:\n",
    "                    f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "            print(f\"\\nCheckpoint saved: {checkpoint_file} ({len(predictions)} predictions)\")\n",
    "            print(f\"Latest prediction (first 100 chars): {response.strip()[:100]}...\")\n",
    "\n",
    "    print(f\"\\nGenerated {len(predictions)} predictions successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError occurred: {e}\")\n",
    "    print(f\"Predictions saved up to example {len(predictions)}\")\n",
    "    print(f\"To resume, simply re-run this cell - it will load from the last checkpoint\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabad83",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to JSONL file\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for pred in predictions:\n",
    "        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Predictions saved to {OUTPUT_FILE}\")\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2425d",
   "metadata": {},
   "source": [
    "### Preview Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45698b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few example predictions\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in [0, 1, 2]:  # Show first 3 examples\n",
    "    if i < len(predictions):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"IPIS ID: {predictions[i]['ipis_id']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"SOURCE (first 200 chars):\\n{predictions[i]['source'][:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"TARGET (first 200 chars):\\n{predictions[i]['target'][:200]}...\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a9e9a",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some basic statistics\n",
    "import numpy as np\n",
    "\n",
    "source_lengths = [len(p['source']) for p in predictions]\n",
    "target_lengths = [len(p['target']) for p in predictions]\n",
    "\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print()\n",
    "print(\"Source text lengths (characters):\")\n",
    "print(f\"  Mean: {np.mean(source_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(source_lengths):.1f}\")\n",
    "print(f\"  Min: {np.min(source_lengths)}\")\n",
    "print(f\"  Max: {np.max(source_lengths)}\")\n",
    "print()\n",
    "print(\"Target text lengths (characters):\")\n",
    "print(f\"  Mean: {np.mean(target_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(target_lengths):.1f}\")\n",
    "print(f\"  Min: {np.min(target_lengths)}\")\n",
    "print(f\"  Max: {np.max(target_lengths)}\")\n",
    "print()\n",
    "print(f\"Average length increase: {np.mean(target_lengths) - np.mean(source_lengths):.1f} chars ({(np.mean(target_lengths) / np.mean(source_lengths) - 1) * 100:.1f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
