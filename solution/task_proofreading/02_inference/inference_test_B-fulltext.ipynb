{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463ab84e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#  Model configuration - adjust to match your trained model\n",
    "MODEL_SIZE = \"8B\"\n",
    "LORA_RANK = 64\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 10\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "\n",
    "# Path to your trained model - automatically find the best checkpoint\n",
    "MODEL_DIR = f\"../../../outputs/qwen3_{MODEL_SIZE}_polish_inclusive_proofreading_fullconv_lora_r{LORA_RANK}_lr{LEARNING_RATE}_ep{EPOCHS}_bs{BATCH_SIZE}_ga{GRADIENT_ACCUMULATION_STEPS}_warmup{WARMUP_STEPS}_seq{MAX_SEQ_LENGTH}\"\n",
    "MODEL_DIR = os.path.abspath(MODEL_DIR)\n",
    "\n",
    "# Find the best checkpoint by scanning all checkpoint directories\n",
    "import json\n",
    "import glob\n",
    "\n",
    "print(f\"Searching for best checkpoint in {MODEL_DIR}...\")\n",
    "\n",
    "best_checkpoint = None\n",
    "best_eval_loss = float('inf')\n",
    "\n",
    "# Find all checkpoint directories\n",
    "checkpoint_dirs = glob.glob(os.path.join(MODEL_DIR, \"checkpoint-*\"))\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    raise FileNotFoundError(f\"No checkpoint directories found in {MODEL_DIR}\")\n",
    "\n",
    "# Scan each checkpoint for trainer_state.json\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, \"trainer_state.json\")\n",
    "    \n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        # Look for eval_loss in log_history\n",
    "        for log_entry in trainer_state.get('log_history', []):\n",
    "            if 'eval_loss' in log_entry:\n",
    "                eval_loss = log_entry['eval_loss']\n",
    "                \n",
    "                if eval_loss < best_eval_loss:\n",
    "                    best_eval_loss = eval_loss\n",
    "                    best_checkpoint = os.path.basename(checkpoint_dir)\n",
    "\n",
    "if best_checkpoint:\n",
    "    MODEL_PATH = os.path.join(MODEL_DIR, best_checkpoint)\n",
    "    print(f\"Found best checkpoint: {best_checkpoint} (eval_loss: {best_eval_loss:.4f})\")\n",
    "else:\n",
    "    # Fallback to the last checkpoint by number\n",
    "    checkpoint_numbers = [int(d.split('-')[-1]) for d in checkpoint_dirs if d.split('-')[-1].isdigit()]\n",
    "    if checkpoint_numbers:\n",
    "        last_checkpoint_num = max(checkpoint_numbers)\n",
    "        best_checkpoint = f\"checkpoint-{last_checkpoint_num}\"\n",
    "        MODEL_PATH = os.path.join(MODEL_DIR, best_checkpoint)\n",
    "        print(f\"Warning: No eval_loss found. Using last checkpoint: {best_checkpoint}\")\n",
    "    else:\n",
    "        raise ValueError(\"Could not find any valid checkpoints\")\n",
    "\n",
    "print(f\"Using model from: {MODEL_PATH}\")\n",
    "\n",
    "# Inference parameters\n",
    "TEMPERATURE = 0.3  # Lower temperature for more precise transformations\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "MAX_NEW_TOKENS = 4096  # Enough for longer texts\n",
    "\n",
    "# File paths\n",
    "TEST_FILE = \"../../../data/taskA/test_B.jsonl\"\n",
    "OUTPUT_FILE = f\"predictions_test_B-fulltext-lora_r{LORA_RANK}.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc0434",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix HuggingFace cache permissions\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = os.path.expanduser('~/.cache/huggingface')\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.path.expanduser('~/.cache/huggingface/transformers')\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.expanduser('~/.cache/huggingface/datasets')\n",
    "os.environ['TRITON_CACHE_DIR'] = os.path.expanduser('~/.cache/triton')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d3785",
   "metadata": {},
   "source": [
    "### Load the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Enable inference mode for 2x faster generation\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448e741",
   "metadata": {},
   "source": [
    "### Load System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Polish system prompt used during training\n",
    "with open('../../../system_prompts/proofreading/system_prompt_pl_proofreading', 'r', encoding='utf-8') as f:\n",
    "    SYSTEM_PROMPT = f.read().strip()\n",
    "\n",
    "print(\"System prompt loaded.\")\n",
    "print(f\"System prompt length: {len(SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffff057",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load test data\n",
    "test_data = load_jsonl(TEST_FILE)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test examples from {TEST_FILE}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"IPIS ID: {test_data[0]['ipis_id']}\")\n",
    "print(f\"Prompt: {test_data[0]['prompt'][:100]}...\")\n",
    "print(f\"Source: {test_data[0]['source'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9da15d",
   "metadata": {},
   "source": [
    "### Generate Predictions\n",
    "\n",
    "This will generate gender-inclusive versions for all texts in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96df53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR_PREFIX = f\"inference_checkpoints_fulltext_lora_r{LORA_RANK}\"\n",
    "\n",
    "SAVE_INTERVAL = 25\n",
    "NUM_PASSES = 3\n",
    "\n",
    "for pass_num in tqdm(range(1, NUM_PASSES + 1), desc=\"Overall Passes\", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'):\n",
    "    print(f\"\\n=== Starting inference pass {pass_num}/{NUM_PASSES} ===\")\n",
    "    CHECKPOINT_DIR = f\"{CHECKPOINT_DIR_PREFIX}/pass_{pass_num}\"\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "    # Check for existing checkpoints to resume from\n",
    "    checkpoint_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"predictions_checkpoint_\")])\n",
    "    final_checkpoint_files = sorted([f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"predictions_final_\")])\n",
    "\n",
    "    if final_checkpoint_files:\n",
    "        print(\"Final checkpoint already exists for this pass. Skipping to next pass.\")\n",
    "        continue\n",
    "\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "        print(\"Loading predictions from checkpoint...\")\n",
    "        \n",
    "        with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "            predictions = [json.loads(line) for line in f]\n",
    "        \n",
    "        processed_ids = {p['ipis_id'] for p in predictions}\n",
    "        start_idx = len(predictions)\n",
    "        print(f\"Resuming from example {start_idx} ({len(predictions)} already processed)\")\n",
    "    else:\n",
    "        predictions = []\n",
    "        processed_ids = set()\n",
    "        start_idx = 0\n",
    "        print(\"Starting from scratch\")\n",
    "\n",
    "    print(f\"\\nGenerating predictions for {len(test_data)} examples...\")\n",
    "    print(f\"Parameters: temperature={TEMPERATURE}, top_p={TOP_P}, max_new_tokens={MAX_NEW_TOKENS}\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        for idx, item in enumerate(tqdm(test_data[start_idx:], initial=start_idx, total=len(test_data), desc=\"Generating predictions\", leave=False, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')):\n",
    "            # Skip if already processed\n",
    "            if item['ipis_id'] in processed_ids:\n",
    "                continue\n",
    "            \n",
    "            # Construct the prompt using the same format as training\n",
    "            user_message = item['prompt'] + item['source']\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize and move to GPU\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            # Generate prediction\n",
    "            with torch.inference_mode():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    temperature=TEMPERATURE,\n",
    "                    top_p=TOP_P,\n",
    "                    top_k=TOP_K,\n",
    "                    do_sample=True if TEMPERATURE > 0 else False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            # Decode only the generated tokens (excluding the input prompt)\n",
    "            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "            response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up any remaining chat template artifacts\n",
    "            if \"<|im_end|>\" in response:\n",
    "                response = response.split(\"<|im_end|>\")[0]\n",
    "            if \"<|im_start|>\" in response:\n",
    "                response = response.split(\"<|im_start|>\")[-1]\n",
    "                if \"\\n\" in response:\n",
    "                    response = response.split(\"\\n\", 1)[-1]\n",
    "            \n",
    "            # For debugging: also save the full decoded output\n",
    "            full_generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Create prediction entry in the same format as sample_proofreading.jsonl\n",
    "            prediction = {\n",
    "                \"ipis_id\": item['ipis_id'],\n",
    "                \"source\": item['source'],\n",
    "                \"target\": response.strip(),\n",
    "                \"normalised_target\": None,  # Will be filled in later normalization step\n",
    "                \"full_generated_text\": full_generated_text  # Save complete output for debugging\n",
    "            }\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            processed_ids.add(item['ipis_id'])\n",
    "            \n",
    "            # Save checkpoint every 10 examples\n",
    "            if len(predictions) % SAVE_INTERVAL == 0:\n",
    "                checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"predictions_checkpoint_{len(predictions):05d}.jsonl\")\n",
    "                with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                    for pred in predictions:\n",
    "                        f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "                print(f\"\\nCheckpoint saved: {checkpoint_file} ({len(predictions)} predictions)\")\n",
    "                # print(f\"Latest prediction (first 100 chars): {response.strip()[:100]}...\")\n",
    "\n",
    "        print(f\"\\nGenerated {len(predictions)} predictions successfully!\")\n",
    "        # clear the cell output to reduce clutter\n",
    "        checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"predictions_final_{len(predictions):05d}.jsonl\")\n",
    "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "            for pred in predictions:\n",
    "                f.write(json.dumps(pred, ensure_ascii=False) + '\\n')\n",
    "        print(f\"\\nCheckpoint saved: {checkpoint_file} ({len(predictions)} predictions)\")\n",
    "        from IPython.display import clear_output\n",
    "        # clear_output()\n",
    "        print(f\"Completed pass {pass_num}/{NUM_PASSES}.\")\n",
    "    \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Error occurred: {e}\")\n",
    "        print(f\"Predictions saved up to example {len(predictions)}\")\n",
    "        print(f\"To resume, simply re-run this cell - it will load from the last checkpoint\")\n",
    "        raise\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poleval-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
